{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "0ukP3ituDSCG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"center\">\n",
    "    <h3>\n",
    "        بسم الله الرحمن الرحیم\n",
    "    </h3>\n",
    "    <br>\n",
    "    <h1>\n",
    "        <strong>\n",
    "            بازیابی پیشرفته اطلاعات\n",
    "        </strong>\n",
    "    </h1>\n",
    "    <h2>\n",
    "        <strong>\n",
    "            تمرین سوم (موتور جستجوی اخبار)\n",
    "        </strong>\n",
    "    </h2>\n",
    "    <br>\n",
    "    <h3>\n",
    "        محمد هجری - ٩٨١٠٦١٥٦\n",
    "        <br><br>\n",
    "        ارشان دلیلی - ٩٨١٠٥٧٥١\n",
    "        <br><br>\n",
    "        سروش جهان‌زاد - ٩٨١٠٠٣٨٩\n",
    "    </h3>\n",
    "    <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cEdo5ea3j39X"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        دسترسی به داده‌ها و مدل‌های ذخیره شده\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        به دلیل حجم بالای فایل‌های ذخیره شده، از قرار دادن آن‌ها در پوشه نوتبوک صرف نظر کرده و با اجرای قطعه کد زیر، از طریق گوگل درایو به آن‌ها دسترسی پیدا می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vcuDlf6j39Y",
    "outputId": "0e379dab-0d3a-4d06-c1c8-6710a4d0277c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# address = \"University/Term 6/MIR/Homeworks/HW3\"\n",
    "# sys.path.append(f\"/content/drive/My Drive/{address}\")\n",
    "# %cd /content/drive/My\\ Drive/$address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IR0F_wAbDSCJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        نصب و دسترسی به کتابخانه‌های مورد نیاز\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای دو قطعه کد زیر، کتابخانه‌هایی که از آن‌ها در این تمرین استفاده شده است، نصب و قابل استفاده می‌شوند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfxCzBqoDSCK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install bs4\n",
    "# !pip install tqdm\n",
    "# !pip install pandas\n",
    "# !pip install requests\n",
    "# !pip install hazm\n",
    "# !pip install unidecode\n",
    "# !pip install fasttext\n",
    "# !pip install scikit-learn\n",
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install sentence_transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIglKdxHDSCM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import hazm\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aNJYaFC1DSCM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ١. دریافت داده‌ها\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این تمرین، بیش از ٦٨ هزار خبر از\n",
    "        <a href=\"https://www.hamshahrionline.ir/\"> وب‌سایت همشهری‌آنلاین </a>\n",
    "        گردآوری شده که در ١٠ دسته‌ی سیاسی، جهانی، اقتصادی، اجتماعی، شهری، ورزشی، علمی، فرهنگی، فناوری اطلاعات و مهارت‌های زندگی طبقه‌بندی شده‌اند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5D13VU7DSCN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'Politics': 'سیاسی',\n",
    "    'World': 'جهانی',\n",
    "    'Economy': 'اقتصادی',\n",
    "    'Society': 'اجتماعی',\n",
    "    'City': 'شهری',\n",
    "    'Sport': 'ورزشی',\n",
    "    'Science': 'علمی',\n",
    "    'Culture': 'فرهنگی',\n",
    "    'IT': 'فناوری اطلاعات',\n",
    "    'LifeSkills': 'مهارت‌های زندگی',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gviMdkLQDSCN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        برای دریافت داده‌ها یک ماژول Scraper ساخته‌ایم که اخبار مربوط به ١٠ دسته‌ی مذکور را در بازه‌ی زمانی تعیین شده، کراول کرده و در فایل dataset.zip ذخیره و فشرده سازی می‌کند. کد مربوط به این ماژول را در زیر مشاهده می‌کنید.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFUFTo7mDSCQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset_from_file():\n",
    "    dataset = []\n",
    "    with zipfile.ZipFile('dataset.zip', 'r') as zip_file:\n",
    "        zip_file.extractall()\n",
    "    with open('dataset.csv', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            data = dict(zip(header[1:], row[1:]))\n",
    "            dataset.append(data)\n",
    "    os.remove('dataset.csv')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame(read_dataset_from_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OkLqdbKkDSCR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "    با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "ktN_vJjlDSCR",
    "outputId": "edf2e129-d901-4de8-9b56-2383578c4585",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def display_dataset_info():\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    length_dict = {key: 0 for key in CATEGORIES.keys()}\n",
    "    for _, data in dataset.iterrows():\n",
    "        length_dict[data['category']] += 1\n",
    "\n",
    "    df_dict = {\n",
    "        'دسته': CATEGORIES.values(),\n",
    "        'تعداد': length_dict.values(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.index += 1\n",
    "    df.loc[0] = ['کل خبرها', len(dataset)]\n",
    "    df = df.sort_index()\n",
    "    display(df)\n",
    "\n",
    "\n",
    "display_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Bah1P3lNDSCS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ٢. پیش پردازش اولیه‌ی متن\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        ابزار مورد استفاده برای پیش‌پردازش متن ورودی به صورت ماژولار طراحی شده است؛ به طوری که با صدا زدن تابع preprocess از آن، متن داده شده با عبور از یک خط لوله به صورت مرحله به مرحله تغییر می‌کند تا به یک ساختار استاندارد برسد. این مراحل عبارتند از:\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        نرمال سازی داده‌ها (normalize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف لینک‌ها (remove_links)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف نشانه‌های نگارشی (remove_punctuations)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        واحد سازی داده‌ها (word_tokenize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف کلمات نامعتبر (remove_invalid_words)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف ایست‌واژه‌ها (remove_stopwords)\n",
    "    </li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjtCqkWsDSCS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, stopwords_path):\n",
    "        self.stopwords = []\n",
    "        with open(stopwords_path, encoding='utf-8') as file:\n",
    "            self.stopwords = file.read().split()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_links(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        words = self.word_tokenize(text)\n",
    "        words = self.remove_invalid_words(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    def normalize(self, text):\n",
    "        return hazm.Normalizer().normalize(text)\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        patterns = ['\\S*http\\S*', '\\S*www\\S*', '\\S+\\.ir\\S*', '\\S+\\.com\\S*', '\\S+\\.org\\S*', '\\S*@\\S*']\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        return re.sub(f'[{punctuation}؟،٪×÷»«]+', '', text)\n",
    "\n",
    "    def word_tokenize(self, text):\n",
    "        return hazm.word_tokenize(text)\n",
    "\n",
    "    def remove_invalid_words(self, words):\n",
    "        return [word for word in words if len(word) > 3 or re.match('^[\\u0600-\\u06FF]{2,3}$', word)]\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        return [word for word in words if word not in self.stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjH18UKt0b9O"
   },
   "outputs": [],
   "source": [
    "def save_preprocessed_texts(texts, path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(texts, file)\n",
    "\n",
    "\n",
    "def load_preprocessed_texts(path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def data_to_text(data):\n",
    "    return ' '.join([data['title']]).lower()\n",
    "\n",
    "\n",
    "def get_preprocessed_texts(dataset, preprocessor, mode, save=False):\n",
    "    preprocessed_texts = []\n",
    "    if mode == 'process':\n",
    "        texts = [data_to_text(data) for _, data in dataset.iterrows()]\n",
    "        preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    if mode == 'load':\n",
    "        preprocessed_texts = load_preprocessed_texts()\n",
    "    if save:\n",
    "        save_preprocessed_texts(preprocessed_texts)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SQqYXKb1DSCT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Preprocessor ایجاد کرده و شروع به پیش پردازش داده‌ها می‌کنیم، یا داده‌های پیش‌پردازش‌شده‌ی ذخیره‌شده را از فایل مربوطه بازیابی می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cc5G2zWDSCT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(stopwords_path='stopwords.txt')\n",
    "preprocessed_texts = get_preprocessed_texts(dataset, preprocessor, mode='load', save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2ZLEketNj39j",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با توجه به این که حجم مجموعه داده‌ها باعث ایجاد محدودیت حافظه و زمان برای بعضی از مدل‌ها می‌شود، ناچار هستیم کسری از مجموعه‌ی داده‌ها را به عنوان ورودی برای یادگیری به آن‌ها بدهیم. با اجرای قطعه کد زیر، یک مجموعه داده‌ی کوچک‌تر از روی مجموعه داده‌ی اصلی ایجاد می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKMAe5bwDSCU",
    "outputId": "2fc00f8a-3e97-4bf2-b4dd-fe4372533e75",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mini_dataset(len_each_category=400):\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    mini_dataset = []\n",
    "    for category in CATEGORIES.keys():\n",
    "        dataset_by_category = dataset.loc[dataset['category'] == category]\n",
    "        length = min(len_each_category, dataset_by_category.shape[0])\n",
    "        mini_dataset.append(dataset_by_category.sample(length, random_state=1))\n",
    "\n",
    "    mini_dataset = pd.concat(mini_dataset).reset_index(drop=True)\n",
    "    texts = [data_to_text(data) for _, data in mini_dataset.iterrows()]\n",
    "    mini_preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    return mini_dataset, mini_preprocessed_texts\n",
    "\n",
    "\n",
    "mini_dataset, mini_preprocessed_texts = get_mini_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for doc in preprocessed_texts:\n",
    "    vocabulary = vocabulary.union(set(doc))\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.pickle', \"rb\") as fp:\n",
    "            vocabulary = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vectorizer = CountVectorizer(binary=True, vocabulary=vocabulary)\n",
    "        self.vectors = None\n",
    "        self.words = None\n",
    "        self.dense_vectors_df = None\n",
    "\n",
    "    def fit_transform_vectorizer(self, dataset):\n",
    "        self.vectors = self.vectorizer.fit_transform(list(map(lambda doc: ' '.join(doc), dataset)))\n",
    "        self.words = self.vectorizer.get_feature_names_out()\n",
    "        dense_vectors = self.vectors.todense().tolist()\n",
    "        self.dense_vectors_df = pd.DataFrame(dense_vectors, columns=self.words)\n",
    "\n",
    "    def predict(self, query, dataset, k):\n",
    "        query = ' '.join(Preprocessor(stopwords_path='stopwords.txt').preprocess(query))\n",
    "        query_transform = self.vectorizer.transform([query]).todense().tolist()[0]\n",
    "        dense_vectors = self.dense_vectors_df.values.tolist()\n",
    "        df_cosine_sim = list(map(lambda doc: self.cosine_sim(query_transform, doc), dense_vectors))\n",
    "        self.dense_vectors_df['query_sim'] = df_cosine_sim\n",
    "        indices = self.dense_vectors_df.nlargest(k, 'query_sim').index\n",
    "        self.dense_vectors_df = self.dense_vectors_df.drop(columns=['query_sim'])\n",
    "        return dataset.iloc[indices]\n",
    "\n",
    "    def cosine_sim(self, query, doc):\n",
    "        return np.dot(query, doc) / (np.linalg.norm(query) * np.linalg.norm(doc))\n",
    "\n",
    "    def save_TF_IDF_model(self, path=\"TF_IDF_model.pickle\"):\n",
    "        with open(path, \"wb\") as file:\n",
    "            pickle.dump(self.vectors.todense().tolist(), file)\n",
    "\n",
    "    def load_TF_IDF_model(self, path=\"TF_IDF_model.pickle\"):\n",
    "        with open(path, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "    def prepare(self, dataset, mode, save=False):\n",
    "        model = None\n",
    "        if mode == 'train':\n",
    "            self.fit_transform_vectorizer(dataset)\n",
    "            model = self\n",
    "        if mode == 'load':\n",
    "            model = self.load_TF_IDF_model()\n",
    "        if save:\n",
    "            self.save_TF_IDF_model()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_model = TF_IDF(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_model.fit_transform_vectorizer(preprocessed_texts[10000:11000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_model.fit_transform_vectorizer(preprocessed_texts[1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_model.dense_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = TF_IDF_model.vectors.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "UcPa_LP-j39k",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h4 style='direction:rtl;text-align:justify;'>\n",
    "        ١. مدل مبتنی بر روش boolean\n",
    "    </h4>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "    برای ایجاد این مدل، از کلاس CountVectorizer کتابخانه sklearn استفاده می‌کنیم تا به ازای هر کلمه مشخص شود که در کدام داکیومنت‌ها آمده است. توجه کنید که امکان سرچ با استفاده از عملگرهای or و not نیز پیاده‌سازی شده است. کلمات موجود در یک زیرکوئری همگی رابطه‌ی and را دارند. یک لیست از زیرکوئری‌ها نقش or را ایفا می‌کند. در نهایت، برای exclude کردن یک عبارت از نتایج جست و جو، کافی است در ابتدای عبارت مد نظر کاراکتر \"-\" گذاشته شود تا به عنوان not عمل کند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emhsgEWHDSCY"
   },
   "source": [
    "<div>\n",
    "    <h4 style='direction:rtl;text-align:justify;'>\n",
    "        ٤. مدل مبتنی بر میانگین وزن‌دار بردارهای تعبیه، مانند FastText\n",
    "    </h4>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این بخش، از کتاب‌خانه‌ی fasttext استفاده می‌کنیم و با استفاده از روش skipgram و یک فایل شامل داده‌های موجود اخبار، مدل را به صورت unsupervised به یادگیری وامی‌داریم. پارامترهای استفاده شده‌ی دیگر هم در کد مشخصند؛ به طور مثال، کمینه و بیشینه n را برابر ۲ و ۵ در نظر گرفته‌ایم. پس از انجام یادگیری، میانگین امبدینگ‌ها را در یک آرایه‌ی نامپای ذخیره می‌کنیم. تمام این کارها با صدا زدن متد prepare با حالت train\n",
    "        رخ می‌دهند. همچنین می‌توانیم این متد را با حالت load صدا بزنیم تا یک مدل ذخیره شده را از فایل بازیابی کنیم.\n",
    "    </p>\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        علاوه بر این، می‌توانیم با استفاده از متدهای save_FastText_model و load_FastText_model این مدل و جزئیاتش را ذخیره یا بازیابی کنیم. البته اگر بخواهیم عمل ذخیره را بلافاصله پس از پایان یادگیری انجام دهیم هم می‌توانیم مقدار True را به آرگومان save در متد prepare بدهیم.\n",
    "    </p>\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        پس از پایان یادگیری یا بازیابی مدل یادگرفته شده از فایل ذخیره شده، می‌توانیم با استفاده از متد predict به تعداد دلخواه خبر مرتبط به درخواستمان را به دست بیاوریم. برای این کار، مدلمان بردارهای مربوط به درخواست را به دست می‌آورد و با محاسبه‌ی کسینوس‌ها و به دست آوردن میزان شباهت بین درخواست و خبرهای موجود، شبیه‌ترین خبرها را خروجی می‌دهد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6coHV2SJDSCZ"
   },
   "outputs": [],
   "source": [
    "class FastText:\n",
    "\n",
    "    def __init__(self, preprocessor=None, method='skipgram'):\n",
    "        self.method = method\n",
    "        self.mean_embed = []\n",
    "        self.model = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def train(self, texts):\n",
    "        with open('FastText_train.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(list(map(lambda doc: ' '.join(doc), texts))))\n",
    "        self.model = fasttext.train_unsupervised('FastText_train.txt', self.method, minn=2, maxn=5, wordNgrams=10)\n",
    "        os.remove('FastText_train.txt')\n",
    "        self.mean_embed = list(map(lambda doc:\n",
    "                                   np.mean(list(map(lambda word:\n",
    "                                                    self.model.get_word_vector(word), doc)), axis=0), texts))\n",
    "        self.mean_embed = np.array(self.mean_embed)\n",
    "\n",
    "    def predict(self, query, dataset, k):\n",
    "        if self.preprocessor:\n",
    "            query = self.preprocessor.preprocess(query)\n",
    "        if type(query) == str:\n",
    "            query = query.split()\n",
    "        query_embed = np.mean(list(map(lambda word: self.model.get_word_vector(word), query)), axis=0)\n",
    "        dataset_sim = np.array(list(map(lambda doc: self.cosine_sim(query_embed, doc), self.mean_embed)))\n",
    "        idx = np.argsort(-dataset_sim)\n",
    "        return dataset.iloc[list(idx[:k])]\n",
    "\n",
    "    def cosine_sim(self, query, doc):\n",
    "        return np.dot(query, doc) / (np.linalg.norm(query) * np.linalg.norm(doc))\n",
    "\n",
    "    def save_FastText_model(self, path='FastText_model.bin'):\n",
    "        self.model.save_model(path)\n",
    "        np.save('FastText_mean_embed.npy', self.mean_embed)\n",
    "\n",
    "    def load_FastText_model(self, path=\"FastText_model.bin\"):\n",
    "        self.model = fasttext.load_model(path)\n",
    "        self.mean_embed = np.load('FastText_mean_embed.npy')\n",
    "\n",
    "    def prepare(self, dataset, mode, save=False):\n",
    "        if mode == 'train':\n",
    "            self.train(dataset)\n",
    "        if mode == 'load':\n",
    "            self.load_FastText_model()\n",
    "        if save:\n",
    "            self.save_FastText_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "w12YOYKdj39x"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از مدل FastText ایجاد کرده و شروع به یادگیری داده‌ها و یا خواندن مدل ذخیره شده از روی فایل می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLxjzX57DSCa"
   },
   "outputs": [],
   "source": [
    "FastText_model = FastText()\n",
    "FastText_model.prepare(preprocessed_texts, mode='train', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "PLBbpeaxj39x"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، نتیجه‌ی پردازش کوئری‌های مشخص شده را مشاهده می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در نهایت، سرچ بر اساس عنوان خبر را پیاده سازی می‌کنیم. این سرچ نیازی به استفاده از مدل‌های زبانی ندارد و صرفاً مانند یک فیلتر عمل می‌کند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_CLASSES = {\n",
    "    'Politics': 0,\n",
    "    'World': 1,\n",
    "    'Economy': 2,\n",
    "    'Society': 3,\n",
    "    'City': 4,\n",
    "    'Sport': 5,\n",
    "    'Science': 6,\n",
    "    'Culture': 7,\n",
    "    'IT': 8,\n",
    "    'LifeSkills': 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['category_code'] = dataset['category'].apply(lambda x: CATEGORIES_CLASSES[x])\n",
    "labels = dataset['category_code'].to_numpy()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset['category_code'] = mini_dataset['category'].apply(lambda x: CATEGORIES_CLASSES[x])\n",
    "labels = mini_dataset['category_code'].to_numpy()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(labels))\n",
    "perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.85 * len(labels))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectors[perm[:train_size]]\n",
    "train_labels = labels[perm[:train_size]]\n",
    "test_vectors = vectors[perm[train_size:]]\n",
    "test_labels = labels[perm[train_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clf.predict(test_vectors)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test_labels, y, labels=np.arange(10), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(vectors[40000:40005, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "t76h-25dj39y"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        از این که وقت ارزشمند خود را برای بررسی این نوتبوک صرف کرده‌اید، صمیمانه سپاسگزاریم. (:\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(n_clusters=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = k_means.labels_\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.scatter(np.arange(100), y[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "News-Search-Engine.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
