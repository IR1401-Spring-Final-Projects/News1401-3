{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "0ukP3ituDSCG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"center\">\n",
    "    <h3>\n",
    "        بسم الله الرحمن الرحیم\n",
    "    </h3>\n",
    "    <br>\n",
    "    <h1>\n",
    "        <strong>\n",
    "            بازیابی پیشرفته اطلاعات\n",
    "        </strong>\n",
    "    </h1>\n",
    "    <h2>\n",
    "        <strong>\n",
    "            تمرین چهارم (طبقه‌بندی و خوشه‌بندی)\n",
    "        </strong>\n",
    "    </h2>\n",
    "    <br>\n",
    "    <h3>\n",
    "        محمد هجری - ٩٨١٠٦١٥٦\n",
    "        <br><br>\n",
    "        ارشان دلیلی - ٩٨١٠٥٧٥١\n",
    "        <br><br>\n",
    "        سروش جهان‌زاد - ٩٨١٠٠٣٨٩\n",
    "    </h3>\n",
    "    <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cEdo5ea3j39X"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        دسترسی به داده‌ها و مدل‌های ذخیره شده\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        به دلیل حجم بالای فایل‌های ذخیره شده، از قرار دادن آن‌ها در پوشه نوتبوک صرف نظر کرده و با اجرای قطعه کد زیر، از طریق گوگل درایو به آن‌ها دسترسی پیدا می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vcuDlf6j39Y",
    "outputId": "0e379dab-0d3a-4d06-c1c8-6710a4d0277c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# address = \"University/Term 6/MIR/Homeworks/HW3\"\n",
    "# sys.path.append(f\"/content/drive/My Drive/{address}\")\n",
    "# %cd /content/drive/My\\ Drive/$address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IR0F_wAbDSCJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        نصب و دسترسی به کتابخانه‌های مورد نیاز\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای دو قطعه کد زیر، کتابخانه‌هایی که از آن‌ها در این تمرین استفاده شده است، نصب و قابل استفاده می‌شوند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfxCzBqoDSCK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install bs4\n",
    "# !pip install tqdm\n",
    "# !pip install pandas\n",
    "# !pip install requests\n",
    "# !pip install hazm\n",
    "# !pip install unidecode\n",
    "# !pip install fasttext\n",
    "# !pip install scikit-learn\n",
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install sentence_transformers\n",
    "# !pip install faiss-cpu\n",
    "# !pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sIglKdxHDSCM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import hazm\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aNJYaFC1DSCM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ١. دریافت داده‌ها\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این تمرین، بیش از ٦٨ هزار خبر از\n",
    "        <a href=\"https://www.hamshahrionline.ir/\"> وب‌سایت همشهری‌آنلاین </a>\n",
    "        گردآوری شده که در ١٠ دسته‌ی سیاسی، جهانی، اقتصادی، اجتماعی، شهری، ورزشی، علمی، فرهنگی، فناوری اطلاعات و مهارت‌های زندگی طبقه‌بندی شده‌اند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I5D13VU7DSCN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'Politics': 'سیاسی',\n",
    "    'World': 'جهانی',\n",
    "    'Economy': 'اقتصادی',\n",
    "    'Society': 'اجتماعی',\n",
    "    'City': 'شهری',\n",
    "    'Sport': 'ورزشی',\n",
    "    'Science': 'علمی',\n",
    "    'Culture': 'فرهنگی',\n",
    "    'IT': 'فناوری اطلاعات',\n",
    "    'LifeSkills': 'مهارت‌های زندگی',\n",
    "}\n",
    "\n",
    "CATEGORIES_CLASSES = {\n",
    "    'Politics': 0,\n",
    "    'World': 1,\n",
    "    'Economy': 2,\n",
    "    'Society': 3,\n",
    "    'City': 4,\n",
    "    'Sport': 5,\n",
    "    'Science': 6,\n",
    "    'Culture': 7,\n",
    "    'IT': 8,\n",
    "    'LifeSkills': 9,\n",
    "}\n",
    "\n",
    "CLASSES_CATEGORIES = {v: k for k, v in CATEGORIES_CLASSES.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gviMdkLQDSCN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        برای دریافت داده‌ها یک ماژول Scraper ساخته‌ایم که اخبار مربوط به ١٠ دسته‌ی مذکور را در بازه‌ی زمانی تعیین شده، کراول کرده و در فایل dataset.zip ذخیره و فشرده سازی می‌کند. کد مربوط به این ماژول را در زیر مشاهده می‌کنید.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bFUFTo7mDSCQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset_from_file():\n",
    "    dataset = []\n",
    "    with zipfile.ZipFile('dataset.zip', 'r') as zip_file:\n",
    "        zip_file.extractall()\n",
    "    with open('dataset.csv', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            data = dict(zip(header[1:], row[1:]))\n",
    "            dataset.append(data)\n",
    "    os.remove('dataset.csv')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame(read_dataset_from_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OkLqdbKkDSCR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "    با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "ktN_vJjlDSCR",
    "outputId": "edf2e129-d901-4de8-9b56-2383578c4585",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>دسته</th>\n",
       "      <th>تعداد</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>کل خبرها</td>\n",
       "      <td>68362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سیاسی</td>\n",
       "      <td>15798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>جهانی</td>\n",
       "      <td>2895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اقتصادی</td>\n",
       "      <td>8900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اجتماعی</td>\n",
       "      <td>13585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>شهری</td>\n",
       "      <td>3853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ورزشی</td>\n",
       "      <td>8348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>علمی</td>\n",
       "      <td>3190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>فرهنگی</td>\n",
       "      <td>6512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>فناوری اطلاعات</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>مهارت‌های زندگی</td>\n",
       "      <td>4844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               دسته  تعداد\n",
       "0          کل خبرها  68362\n",
       "1             سیاسی  15798\n",
       "2             جهانی   2895\n",
       "3           اقتصادی   8900\n",
       "4           اجتماعی  13585\n",
       "5              شهری   3853\n",
       "6             ورزشی   8348\n",
       "7              علمی   3190\n",
       "8            فرهنگی   6512\n",
       "9    فناوری اطلاعات    437\n",
       "10  مهارت‌های زندگی   4844"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_dataset_info():\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    length_dict = {key: 0 for key in CATEGORIES.keys()}\n",
    "    for _, data in dataset.iterrows():\n",
    "        length_dict[data['category']] += 1\n",
    "\n",
    "    df_dict = {\n",
    "        'دسته': CATEGORIES.values(),\n",
    "        'تعداد': length_dict.values(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.index += 1\n",
    "    df.loc[0] = ['کل خبرها', len(dataset)]\n",
    "    df = df.sort_index()\n",
    "    display(df)\n",
    "\n",
    "\n",
    "display_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Bah1P3lNDSCS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ٢. پیش پردازش اولیه‌ی متن\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        ابزار مورد استفاده برای پیش‌پردازش متن ورودی به صورت ماژولار طراحی شده است؛ به طوری که با صدا زدن تابع preprocess از آن، متن داده شده با عبور از یک خط لوله به صورت مرحله به مرحله تغییر می‌کند تا به یک ساختار استاندارد برسد. این مراحل عبارتند از:\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        نرمال سازی داده‌ها (normalize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف لینک‌ها (remove_links)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف نشانه‌های نگارشی (remove_punctuations)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        واحد سازی داده‌ها (word_tokenize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف کلمات نامعتبر (remove_invalid_words)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف ایست‌واژه‌ها (remove_stopwords)\n",
    "    </li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SjtCqkWsDSCS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, stopwords_path):\n",
    "        self.stopwords = []\n",
    "        with open(stopwords_path, encoding='utf-8') as file:\n",
    "            self.stopwords = file.read().split()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_links(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        words = self.word_tokenize(text)\n",
    "        words = self.remove_invalid_words(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    def normalize(self, text):\n",
    "        return hazm.Normalizer().normalize(text)\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        patterns = ['\\S*http\\S*', '\\S*www\\S*', '\\S+\\.ir\\S*', '\\S+\\.com\\S*', '\\S+\\.org\\S*', '\\S*@\\S*']\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        return re.sub(f'[{punctuation}؟،٪×÷»«]+', '', text)\n",
    "\n",
    "    def word_tokenize(self, text):\n",
    "        return hazm.word_tokenize(text)\n",
    "\n",
    "    def remove_invalid_words(self, words):\n",
    "        return [word for word in words if len(word) > 3 or re.match('^[\\u0600-\\u06FF]{2,3}$', word)]\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        return [word for word in words if word not in self.stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BjH18UKt0b9O"
   },
   "outputs": [],
   "source": [
    "def save_preprocessed_texts(texts, path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(texts, file)\n",
    "\n",
    "\n",
    "def load_preprocessed_texts(path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def data_to_text(data):\n",
    "    return ' '.join([data['title']]).lower()\n",
    "\n",
    "\n",
    "def get_preprocessed_texts(dataset, preprocessor, mode, save=False):\n",
    "    preprocessed_texts = []\n",
    "    if mode == 'process':\n",
    "        texts = [data_to_text(data) for _, data in dataset.iterrows()]\n",
    "        preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    if mode == 'load':\n",
    "        preprocessed_texts = load_preprocessed_texts()\n",
    "    if save:\n",
    "        save_preprocessed_texts(preprocessed_texts)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SQqYXKb1DSCT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Preprocessor ایجاد کرده و شروع به پیش پردازش داده‌ها می‌کنیم، یا داده‌های پیش‌پردازش‌شده‌ی ذخیره‌شده را از فایل مربوطه بازیابی می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8cc5G2zWDSCT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(stopwords_path='stopwords.txt')\n",
    "preprocessed_texts = get_preprocessed_texts(dataset, preprocessor, mode='load', save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2ZLEketNj39j",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با توجه به این که حجم مجموعه داده‌ها باعث ایجاد محدودیت حافظه و زمان برای بعضی از مدل‌ها می‌شود، ناچار هستیم کسری از مجموعه‌ی داده‌ها را به عنوان ورودی برای یادگیری به آن‌ها بدهیم. با اجرای قطعه کد زیر، یک مجموعه داده‌ی کوچک‌تر از روی مجموعه داده‌ی اصلی ایجاد می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKMAe5bwDSCU",
    "outputId": "2fc00f8a-3e97-4bf2-b4dd-fe4372533e75",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9437/9437 [00:02<00:00, 4148.33it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_mini_dataset(len_each_category=400):\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    mini_dataset = []\n",
    "    for category in CATEGORIES.keys():\n",
    "        dataset_by_category = dataset.loc[dataset['category'] == category]\n",
    "        length = min(len_each_category, dataset_by_category.shape[0])\n",
    "        mini_dataset.append(dataset_by_category.sample(length, random_state=1))\n",
    "\n",
    "    mini_dataset = pd.concat(mini_dataset).reset_index(drop=True)\n",
    "    texts = [data_to_text(data) for _, data in mini_dataset.iterrows()]\n",
    "    mini_preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    return mini_dataset, mini_preprocessed_texts\n",
    "\n",
    "\n",
    "mini_dataset, mini_preprocessed_texts = get_mini_dataset(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset['category_code'] = mini_dataset['category'].apply(lambda x: CATEGORIES_CLASSES[x])\n",
    "labels = mini_dataset['category_code'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "\n",
    "with open('vocabulary.pickle', \"rb\") as fp:\n",
    "            vocabulary = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "        self.vectors = None\n",
    "\n",
    "    def fit_transform_vectorizer(self, dataset):\n",
    "        self.vectors = self.vectorizer.fit_transform(list(map(lambda doc: ' '.join(doc), dataset)))\n",
    "        self.words = self.vectorizer.get_feature_names_out()\n",
    "        dense_vectors = self.vectors.todense().tolist()\n",
    "        return np.array(dense_vectors), self.words\n",
    "\n",
    "    def save_TF_IDF_model(self, path=\"TF_IDF_model.pickle\"):\n",
    "        with open(path, \"wb\") as file:\n",
    "            pickle.dump(self.vectors.todense().tolist(), file)\n",
    "\n",
    "    def load_TF_IDF_model(self, path=\"TF_IDF_model.pickle\"):\n",
    "        with open(path, \"rb\") as file:\n",
    "            return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_model = TF_IDF(vocabulary)\n",
    "vectors, features = TF_IDF_model.fit_transform_vectorizer(mini_preprocessed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors, labels,  test_size=0.15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000, random_state=0)\n",
    "clf.fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IT'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'دعوای اردوغان و گلزار'\n",
    "query = 'سرقت گوشی آیفون با نمایشگر اولد از کیف پوتین'\n",
    "\n",
    "\n",
    "q_em = TF_IDF_model.vectorizer.transform([query])\n",
    "\n",
    "p = clf.predict(q_em)\n",
    "CLASSES_CATEGORIES[p[0]]\n",
    "#p = clf.predict_proba(q_em)\n",
    "#p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accurracy: 0.732\n",
      "F1_Macro: 0.72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Politics</th>\n",
       "      <th>World</th>\n",
       "      <th>Economy</th>\n",
       "      <th>Society</th>\n",
       "      <th>City</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Science</th>\n",
       "      <th>Culture</th>\n",
       "      <th>IT</th>\n",
       "      <th>LifeSkills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Politics</th>\n",
       "      <td>91</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>World</th>\n",
       "      <td>14</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economy</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Society</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Culture</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IT</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LifeSkills</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Politics  World  Economy  Society  City  Sport  Science  Culture  \\\n",
       "Politics          91     18        3        4     7      2        4        7   \n",
       "World             14    105        1        8     1      8       12       10   \n",
       "Economy           12      6      118       15     7      0        2        1   \n",
       "Society            4      1        5      100    15      0       10       16   \n",
       "City               6      1        7        6   112      1        5        9   \n",
       "Sport              4      0        0        4     1    147        0        1   \n",
       "Science            0      4        0        4     0      0       95        9   \n",
       "Culture            5      2        1        2     1      1        6      116   \n",
       "IT                 3      6        4        2     2      1       14        5   \n",
       "LifeSkills         0      2        0        3     0      2       10        8   \n",
       "\n",
       "            IT  LifeSkills  \n",
       "Politics     0           2  \n",
       "World        2           2  \n",
       "Economy      2           0  \n",
       "Society      1           5  \n",
       "City         0           3  \n",
       "Sport        0           1  \n",
       "Science      4          18  \n",
       "Culture      0           3  \n",
       "IT          33           1  \n",
       "LifeSkills   1         119  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_acc = clf.score(test_vectors, test_labels)\n",
    "y = clf.predict(test_vectors)\n",
    "f1_macro = f1_score(test_labels, y, labels=np.arange(10), average='macro')\n",
    "conf_matrix = confusion_matrix(test_labels, y, labels=np.arange(10))\n",
    "print(f\"Mean Accurracy: {round(mean_acc, 3)}\")\n",
    "print(f\"F1_Macro: {round(f1_macro, 3)}\")\n",
    "c_m_df = pd.DataFrame(conf_matrix).rename(columns=CLASSES_CATEGORIES, index=CLASSES_CATEGORIES)\n",
    "c_m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "News-Search-Engine.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
