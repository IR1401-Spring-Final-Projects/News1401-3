{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"center\">\n",
    "    <h3>\n",
    "        بسم الله الرحمن الرحیم\n",
    "    </h3>\n",
    "    <br>\n",
    "    <h1>\n",
    "        <strong>\n",
    "            بازیابی پیشرفته اطلاعات\n",
    "        </strong>\n",
    "    </h1>\n",
    "    <h2>\n",
    "        <strong>\n",
    "            تمرین سوم (موتور جستجوی اخبار)\n",
    "        </strong>\n",
    "    </h2>\n",
    "    <br>\n",
    "    <h3>\n",
    "        محمد هجری - ٩٨١٠٦١٥٦\n",
    "        <br><br>\n",
    "        ارشان دلیلی - ٩٨١٠٥٧٥١\n",
    "        <br><br>\n",
    "        سروش جهان‌زاد - ٩٨١٠٠٣٨٩\n",
    "    </h3>\n",
    "    <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        نصب و دسترسی به کتابخانه‌های مورد نیاز\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای دو قطعه کد زیر، کتابخانه‌هایی که از آن‌ها در این تمرین استفاده شده است، نصب و قابل استفاده می‌شوند.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install bs4\n",
    "# !pip install tqdm\n",
    "# !pip install pandas\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "    <h2 style='direction:rtl;text-align:justify;'>\n",
    "        ١. دریافت داده‌ها\n",
    "    </h2>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این تمرین، بیش از ٨٠ هزار خبر از\n",
    "        <a href=\"https://www.hamshahrionline.ir/\"> وب‌سایت همشهری‌آنلاین </a>\n",
    "        گردآوری شده که در ١٠ دسته‌ی سیاسی، جهانی، اقتصادی، اجتماعی، شهری، ورزشی، علمی، فرهنگی، فناوری اطلاعات و مهارت‌های زندگی طبقه‌بندی شده‌اند.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'Politics': 'سیاسی',\n",
    "    'World': 'جهانی',\n",
    "    'Economy': 'اقتصادی',\n",
    "    'Society': 'اجتماعی',\n",
    "    'City': 'شهری',\n",
    "    'Sport': 'ورزشی',\n",
    "    'Science': 'علمی',\n",
    "    'Culture': 'فرهنگی',\n",
    "    'IT': 'فناوری اطلاعات',\n",
    "    'LifeSkills': 'مهارت‌های زندگی',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        برای دریافت داده‌ها یک ماژول Scraper ساخته‌ایم که اخبار مربوط به ١٠ دسته‌ی مذکور را در بازه‌ی زمانی تعیین شده، کراول کرده و در فایل dataset.zip ذخیره و فشرده سازی می‌کند. کد مربوط به این ماژول را در زیر مشاهده می‌کنید.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self, current_year, current_month):\n",
    "        self.current_year = current_year\n",
    "        self.current_month = current_month\n",
    "\n",
    "    def get_URL_content(self, URL):\n",
    "        while True:\n",
    "            try:\n",
    "                return requests.get(URL, timeout=5).content\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def generate_page_URL(self, page_index, category, year, month):\n",
    "        tp = {'Politics': 6, 'World': 11, 'Economy': 10, 'Society': 5, 'City': 7,\n",
    "              'Sport': 9, 'Science': 20, 'Culture': 26, 'IT': 718, 'LifeSkills': 21}[category]\n",
    "        return f'https://www.hamshahrionline.ir/archive?pi={page_index}&tp={tp}&ty=1&ms=0&mn={month}&yr={year}'\n",
    "\n",
    "    def get_page_URLs_by_time(self, category, year, month):\n",
    "        URLs = []\n",
    "        page_index = 1\n",
    "        while True:\n",
    "            URL = self.generate_page_URL(page_index, category, year, month)\n",
    "            content = self.get_URL_content(URL)\n",
    "            if re.findall('pagination', str(content)):\n",
    "                URLs.append(URL)\n",
    "                page_index += 1\n",
    "            else:\n",
    "                break\n",
    "        return URLs\n",
    "\n",
    "    def get_page_URLs_since(self, category, year, month):\n",
    "        URLs = []\n",
    "        with tqdm() as pbar:\n",
    "            while True:\n",
    "                if month > 12:\n",
    "                    month = 1\n",
    "                    year += 1\n",
    "                pbar.set_description(f'[{category}] [Extracting page URLs] [Date: {year}/{month}]')\n",
    "                URLs_by_time = self.get_page_URLs_by_time(category, year, month)\n",
    "                if URLs_by_time:\n",
    "                    for URL in URLs_by_time:\n",
    "                        URLs.append(URL)\n",
    "                    month += 1\n",
    "                elif self.current_year > year or (self.current_year == year and self.current_month > month):\n",
    "                    month += 1\n",
    "                else:\n",
    "                    break\n",
    "        return URLs\n",
    "\n",
    "    def get_news_URLs_since(self, category, year, month):\n",
    "        news_URLs = []\n",
    "        page_URLs = self.get_page_URLs_since(category, year, month)\n",
    "        with tqdm(page_URLs) as pbar:\n",
    "            for page_URL in pbar:\n",
    "                content = self.get_URL_content(page_URL)\n",
    "                soup = BeautifulSoup(content, 'html5lib')\n",
    "                for item in soup.findAll('li', attrs={'class': 'news'}):\n",
    "                    URL = item.find('div', attrs={'class': 'desc'}).find('h3').find('a')['href']\n",
    "                    URL = 'https://www.hamshahrionline.ir' + URL\n",
    "                    news_URLs.append(URL)\n",
    "                pbar.set_description(f'[{category}] [Extracting news URLs] [{len(news_URLs)} news until now]')\n",
    "        return news_URLs\n",
    "\n",
    "    def parse_news(self, URL, category):\n",
    "        try:\n",
    "            content = self.get_URL_content(URL)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            date = soup.find('div', {'class': 'col-6 col-sm-4 col-xl-4 item-date'}).span.text.strip()\n",
    "            title = soup.find('div', {'class': 'item-title'}).h1.text.strip()\n",
    "            intro = soup.find('p', {'class': 'introtext', 'itemprop': 'description'}).text.strip()\n",
    "            body = soup.find('div', {'class': 'item-text', 'itemprop': 'articleBody'}).text.strip()\n",
    "            category_PER = soup.find_all('li', {'class': 'breadcrumb-item'})\n",
    "            category_PER = list(map(lambda x: x.text.strip(), category_PER))[1:]\n",
    "            return {\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'intro': intro,\n",
    "                'body': body,\n",
    "                'category-PER': category_PER,\n",
    "                'category-ENG': category,\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def scrape(self, from_year, from_month):\n",
    "        categories = ['Politics', 'World', 'Economy', 'Society', 'City',\n",
    "                      'Sport', 'Science', 'Culture', 'IT', 'LifeSkills']\n",
    "        news = []\n",
    "        for category in categories:\n",
    "            URLs = self.get_news_URLs_since(category, from_year, from_month)\n",
    "            with tqdm(URLs) as pbar:\n",
    "                pbar.set_description(f'[{category}] [Scraping news]')\n",
    "                for URL in pbar:\n",
    "                    news.append(self.parse_news(URL, category))\n",
    "        news = list(filter(None, news))\n",
    "        pd.DataFrame(news).to_csv(f'dataset.csv', encoding='utf-8')\n",
    "        with zipfile.ZipFile('dataset.zip', 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.write('dataset.csv')\n",
    "        os.remove('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Scraper ایجاد شده و شروع به دریافت و ذخیره‌سازی داده‌ها می‌کند. خبرهای دریافت شده همگی مربوط به قرن جدید، از سال ١٤٠٠ به بعد هستند.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "scraper = Scraper(current_year=1401, current_month=3)\n",
    "# scraper.scrape(from_year=1400, from_month=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        بعد از ذخیره شدن داده‌ها در فایل فشرده dataset.zip، آن‌ها را از این فایل استخراج کرده و وارد برنامه می‌کنیم. با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def read_dataset_from_file():\n",
    "    dataset = []\n",
    "    with zipfile.ZipFile('dataset.zip', 'r') as zip_file:\n",
    "        zip_file.extractall()\n",
    "    with open('dataset.csv', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            data = dict(zip(header[1:], row[1:]))\n",
    "            dataset.append(data)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = read_dataset_from_file()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "    با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def display_dataset_info():\n",
    "    global CATEGORIES\n",
    "\n",
    "    length_dict = {key: 0 for key in CATEGORIES.keys()}\n",
    "    for data in dataset:\n",
    "        length_dict[data['category-ENG']] += 1\n",
    "\n",
    "    df_dict = {\n",
    "        'دسته': CATEGORIES.values(),\n",
    "        'تعداد': length_dict.values(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.index += 1\n",
    "    df.loc[0] = ['کل خبرها', len(dataset)]\n",
    "    df = df.sort_index()\n",
    "    display(df)\n",
    "\n",
    "\n",
    "display_dataset_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "               دسته  تعداد\n0          کل خبرها  81846\n1             سیاسی  16012\n2             جهانی   4847\n3           اقتصادی   9966\n4           اجتماعی  14259\n5              شهری   4323\n6             ورزشی   8486\n7              علمی   3267\n8            فرهنگی   6657\n9    فناوری اطلاعات    444\n10  مهارت‌های زندگی  13585",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>دسته</th>\n      <th>تعداد</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>کل خبرها</td>\n      <td>81846</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>سیاسی</td>\n      <td>16012</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>جهانی</td>\n      <td>4847</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>اقتصادی</td>\n      <td>9966</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اجتماعی</td>\n      <td>14259</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>شهری</td>\n      <td>4323</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ورزشی</td>\n      <td>8486</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>علمی</td>\n      <td>3267</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>فرهنگی</td>\n      <td>6657</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>فناوری اطلاعات</td>\n      <td>444</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>مهارت‌های زندگی</td>\n      <td>13585</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "    <h2 style='direction:rtl;text-align:justify;'>\n",
    "        ٢. پیش پردازش اولیه‌ی متن\n",
    "    </h2>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        عشقی که سید\n",
    "    </p>\n",
    "</div>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}