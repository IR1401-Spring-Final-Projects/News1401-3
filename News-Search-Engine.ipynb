{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"center\">\n",
    "    <h3>\n",
    "        بسم الله الرحمن الرحیم\n",
    "    </h3>\n",
    "    <br>\n",
    "    <h1>\n",
    "        <strong>\n",
    "            بازیابی پیشرفته اطلاعات\n",
    "        </strong>\n",
    "    </h1>\n",
    "    <h2>\n",
    "        <strong>\n",
    "            تمرین سوم (موتور جستجوی اخبار)\n",
    "        </strong>\n",
    "    </h2>\n",
    "    <br>\n",
    "    <h3>\n",
    "        محمد هجری - ٩٨١٠٦١٥٦\n",
    "        <br><br>\n",
    "        ارشان دلیلی - ٩٨١٠٥٧٥١\n",
    "        <br><br>\n",
    "        سروش جهان‌زاد - ٩٨١٠٠٣٨٩\n",
    "    </h3>\n",
    "    <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (1.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.13.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.64.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: html5lib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from html5lib) (1.13.0)\n",
      "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from html5lib) (0.5.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install bs4\n",
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self, current_year, current_month):\n",
    "        self.current_year = current_year\n",
    "        self.current_month = current_month\n",
    "\n",
    "    def get_URL_content(self, URL):\n",
    "        while True:\n",
    "            try:\n",
    "                return requests.get(URL, timeout=5).content\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def generate_page_URL(self, page_index, category, year, month):\n",
    "        tp = {'IranPolitics': 6, 'World': 11, 'Economy': 10, 'Society': 5, 'City': 7,\n",
    "              'LifeSkills': 21, 'IT': 718, 'Science': 20, 'Culture': 26, 'Sport': 9}[category]\n",
    "        return f'https://www.hamshahrionline.ir/archive?pi={page_index}&tp={tp}&ty=1&ms=0&mn={month}&yr={year}'\n",
    "\n",
    "    def get_page_URLs_by_time(self, category, year, month):\n",
    "        URLs = []\n",
    "        page_index = 1\n",
    "        while True:\n",
    "            URL = self.generate_page_URL(page_index, category, year, month)\n",
    "            content = self.get_URL_content(URL)\n",
    "            if re.findall('pagination', str(content)):\n",
    "                URLs.append(URL)\n",
    "                page_index += 1\n",
    "            else:\n",
    "                break\n",
    "        return URLs\n",
    "\n",
    "    def get_page_URLs_since(self, category, year, month):\n",
    "        URLs = []\n",
    "        with tqdm() as pbar:\n",
    "            while True:\n",
    "                if month > 12:\n",
    "                    month = 1\n",
    "                    year += 1\n",
    "                pbar.set_description(f'[{category}] [Extracting page URLs] [Date: {year}/{month}]')\n",
    "                URLs_by_time = self.get_page_URLs_by_time(category, year, month)\n",
    "                if URLs_by_time:\n",
    "                    for URL in URLs_by_time:\n",
    "                        URLs.append(URL)\n",
    "                    month += 1\n",
    "                elif self.current_year > year or (self.current_year == year and self.current_month > month):\n",
    "                    month += 1\n",
    "                else:\n",
    "                    break\n",
    "        return URLs\n",
    "\n",
    "    def get_news_URLs_since(self, category, year, month):\n",
    "        news_URLs = []\n",
    "        page_URLs = self.get_page_URLs_since(category, year, month)\n",
    "        with tqdm(page_URLs) as pbar:\n",
    "            for page_URL in pbar:\n",
    "                content = self.get_URL_content(page_URL)\n",
    "                soup = BeautifulSoup(content, 'html5lib')\n",
    "                for item in soup.findAll('li', attrs={'class': 'news'}):\n",
    "                    URL = item.find('div', attrs={'class': 'desc'}).find('h3').find('a')['href']\n",
    "                    URL = 'https://www.hamshahrionline.ir' + URL\n",
    "                    news_URLs.append(URL)\n",
    "                pbar.set_description(f'[{category}] [Extracting news URLs] [{len(news_URLs)} news until now]')\n",
    "        return news_URLs\n",
    "\n",
    "    def parse_news(self, URL):\n",
    "        try:\n",
    "            content = self.get_URL_content(URL)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            date = soup.find(\"div\", {\"class\": \"col-6 col-sm-4 col-xl-4 item-date\"}).span.text.strip()\n",
    "            title = soup.find(\"div\", {\"class\": \"item-title\"}).h1.text.strip()\n",
    "            intro = soup.find(\"p\", {\"class\": \"introtext\", \"itemprop\": \"description\"}).text.strip()\n",
    "            body = soup.find(\"div\", {\"class\": \"item-text\", \"itemprop\": \"articleBody\"}).text.strip()\n",
    "            category = soup.find_all(\"li\", {\"class\": \"breadcrumb-item\"})\n",
    "            category = list(map(lambda x: x.text.strip(), category))[1:]\n",
    "            return {\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'intro': intro,\n",
    "                'body': body,\n",
    "                'category': category,\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def scrape(self, from_year, from_month):\n",
    "        categories = ['IranPolitics', 'World', 'Economy', 'Society', 'City', 'LifeSkills', 'IT', 'Science', 'Culture', 'Sport']\n",
    "        # categories = ['IranPolitics'] # Mohammad\n",
    "        # categories = ['Society', 'City', 'LifeSkills'] # Arshan\n",
    "        categories = ['IT', 'Science', 'Culture', 'Sport'] # Soroush\n",
    "        # TODO: uncomment yours\n",
    "        category_news = {}\n",
    "        for category in categories:\n",
    "            news = []\n",
    "            URLs = self.get_news_URLs_since(category, from_year, from_month)\n",
    "            with tqdm(URLs) as pbar:\n",
    "                pbar.set_description(f'[{category}] [Scraping news]')\n",
    "                for URL in pbar:\n",
    "                    news.append(self.parse_news(URL))\n",
    "            news = list(filter(None, news))\n",
    "            category_news[category] = news\n",
    "        return category_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper(current_year=1401, current_month=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs = scraper.get_all_urls_since(year=1401, month=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[IT] [Extracting page URLs] [Date: 1401/3]: : 0it [00:07, ?it/s] \n",
      "[IT] [Extracting news URLs] [444 news until now]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:09<00:00,  1.85it/s]\n",
      "[IT] [Scraping news]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 444/444 [05:33<00:00,  1.33it/s]\n",
      "[Science] [Extracting page URLs] [Date: 1401/4]: : 0it [06:35, ?it/s] \n",
      "[Science] [Extracting news URLs] [3267 news until now]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [01:12<00:00,  1.59it/s]\n",
      "[Science] [Scraping news]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3267/3267 [1:05:54<00:00,  1.21s/it]\n",
      "[Culture] [Extracting page URLs] [Date: 1401/4]: : 0it [33:51, ?it/s] \n",
      "[Culture] [Extracting news URLs] [6657 news until now]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 230/230 [1:31:16<00:00, 23.81s/it]\n",
      "[Culture] [Scraping news]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6657/6657 [3:27:50<00:00,  1.87s/it]\n",
      "[Sport] [Extracting page URLs] [Date: 1401/4]: : 0it [04:52, ?it/s] \n",
      "[Sport] [Extracting news URLs] [8486 news until now]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 291/291 [02:49<00:00,  1.71it/s]\n",
      "[Sport] [Scraping news]: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8486/8486 [1:41:09<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "category_news = scraper.scrape(from_year=1400, from_month=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for category, news in category_news.items():\n",
    "    df = pd.DataFrame(news)\n",
    "    df.to_csv(f\"{category}_dataset.csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
