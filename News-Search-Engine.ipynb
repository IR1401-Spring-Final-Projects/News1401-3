{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"center\">\n",
    "    <h3>\n",
    "        بسم الله الرحمن الرحیم\n",
    "    </h3>\n",
    "    <br>\n",
    "    <h1>\n",
    "        <strong>\n",
    "            بازیابی پیشرفته اطلاعات\n",
    "        </strong>\n",
    "    </h1>\n",
    "    <h2>\n",
    "        <strong>\n",
    "            تمرین سوم (موتور جستجوی اخبار)\n",
    "        </strong>\n",
    "    </h2>\n",
    "    <br>\n",
    "    <h3>\n",
    "        محمد هجری - ٩٨١٠٦١٥٦\n",
    "        <br><br>\n",
    "        ارشان دلیلی - ٩٨١٠٥٧٥١\n",
    "        <br><br>\n",
    "        سروش جهان‌زاد - ٩٨١٠٠٣٨٩\n",
    "    </h3>\n",
    "    <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[IranPolitics] [Extracting page URLs] [Date: 1401/4]: : 0it [05:17, ?it/s] \n",
      "[IranPolitics] [Extracting news URLs] [16003 news until now]: 100%|██████████| 540/540 [07:44<00:00,  1.16it/s]\n",
      "[IranPolitics] [Scraping news]:  56%|█████▌    | 8922/16003 [2:25:18<1:55:19,  1.02it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'span'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20304/1464553547.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[0mscraper\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mScraper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcurrent_year\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1401\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcurrent_month\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 101\u001B[1;33m \u001B[0mcategory_news\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mscraper\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscrape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfrom_year\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1400\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_month\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    102\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20304/1464553547.py\u001B[0m in \u001B[0;36mscrape\u001B[1;34m(self, from_year, from_month)\u001B[0m\n\u001B[0;32m     94\u001B[0m                 \u001B[0mpbar\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_description\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'[{category}] [Scraping news]'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m                 \u001B[1;32mfor\u001B[0m \u001B[0mURL\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpbar\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 96\u001B[1;33m                     \u001B[0mnews\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparse_news\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mURL\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     97\u001B[0m             \u001B[0mcategory_news\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcategory\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnews\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     98\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcategory_news\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20304/1464553547.py\u001B[0m in \u001B[0;36mparse_news\u001B[1;34m(self, URL)\u001B[0m\n\u001B[0;32m     67\u001B[0m         \u001B[0mcontent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_URL_content\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mURL\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[0msoup\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBeautifulSoup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcontent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'html.parser'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m         \u001B[0mdate\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msoup\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"div\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m\"class\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"col-6 col-sm-4 col-xl-4 item-date\"\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m         \u001B[0mtitle\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msoup\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"div\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m\"class\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"item-title\"\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mh1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[0mintro\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msoup\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"p\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m\"class\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"introtext\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"itemprop\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"description\"\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'span'"
     ]
    }
   ],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self, current_year, current_month):\n",
    "        self.current_year = current_year\n",
    "        self.current_month = current_month\n",
    "\n",
    "    def get_URL_content(self, URL):\n",
    "        while True:\n",
    "            try:\n",
    "                return requests.get(URL, timeout=10).content\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def generate_page_URL(self, page_index, category, year, month):\n",
    "        tp = {'IranPolitics': 6, 'World': 11, 'Economy': 10, 'Society': 5, 'City': 7,\n",
    "              'LifeSkills': 21, 'IT': 718, 'Science': 20, 'Culture': 26, 'Sport': 9}[category]\n",
    "        return f'https://www.hamshahrionline.ir/archive?pi={page_index}&tp={tp}&ty=1&ms=0&mn={month}&yr={year}'\n",
    "\n",
    "    def get_page_URLs_by_time(self, category, year, month):\n",
    "        URLs = []\n",
    "        page_index = 1\n",
    "        while True:\n",
    "            URL = self.generate_page_URL(page_index, category, year, month)\n",
    "            content = self.get_URL_content(URL)\n",
    "            if re.findall('pagination', str(content)):\n",
    "                URLs.append(URL)\n",
    "                page_index += 1\n",
    "            else:\n",
    "                break\n",
    "        return URLs\n",
    "\n",
    "    def get_page_URLs_since(self, category, year, month):\n",
    "        URLs = []\n",
    "        with tqdm() as pbar:\n",
    "            while True:\n",
    "                if month > 12:\n",
    "                    month = 1\n",
    "                    year += 1\n",
    "                pbar.set_description(f'[{category}] [Extracting page URLs] [Date: {year}/{month}]')\n",
    "                URLs_by_time = self.get_page_URLs_by_time(category, year, month)\n",
    "                if URLs_by_time:\n",
    "                    for URL in URLs_by_time:\n",
    "                        URLs.append(URL)\n",
    "                    month += 1\n",
    "                elif self.current_year > year or (self.current_year == year and self.current_month > month):\n",
    "                    month += 1\n",
    "                else:\n",
    "                    break\n",
    "        return URLs\n",
    "\n",
    "    def get_news_URLs_since(self, category, year, month):\n",
    "        news_URLs = []\n",
    "        page_URLs = self.get_page_URLs_since(category, year, month)\n",
    "        with tqdm(page_URLs) as pbar:\n",
    "            for page_URL in pbar:\n",
    "                content = self.get_URL_content(page_URL)\n",
    "                soup = BeautifulSoup(content, 'html5lib')\n",
    "                for item in soup.findAll('li', attrs={'class': 'news'}):\n",
    "                    URL = item.find('div', attrs={'class': 'desc'}).find('h3').find('a')['href']\n",
    "                    URL = 'https://www.hamshahrionline.ir' + URL\n",
    "                    news_URLs.append(URL)\n",
    "                pbar.set_description(f'[{category}] [Extracting news URLs] [{len(news_URLs)} news until now]')\n",
    "        return news_URLs\n",
    "\n",
    "    def parse_news(self, URL):\n",
    "        try:\n",
    "            content = self.get_URL_content(URL)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            date = soup.find(\"div\", {\"class\": \"col-6 col-sm-4 col-xl-4 item-date\"}).span.text.strip()\n",
    "            title = soup.find(\"div\", {\"class\": \"item-title\"}).h1.text.strip()\n",
    "            intro = soup.find(\"p\", {\"class\": \"introtext\", \"itemprop\": \"description\"}).text.strip()\n",
    "            body = soup.find(\"div\", {\"class\": \"item-text\", \"itemprop\": \"articleBody\"}).text.strip()\n",
    "            category = soup.find_all(\"li\", {\"class\": \"breadcrumb-item\"})\n",
    "            category = list(map(lambda x: x.text.strip(), category))[1:]\n",
    "            return {\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'intro': intro,\n",
    "                'body': body,\n",
    "                'category': category,\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def scrape(self, from_year, from_month):\n",
    "        categories = ['IranPolitics', 'World', 'Economy', 'Society', 'City', 'LifeSkills', 'IT', 'Science', 'Culture', 'Sport']\n",
    "        categories = ['IranPolitics'] # Mohammad\n",
    "        # categories = ['Society', 'City', 'LifeSkills'] # Arshan\n",
    "        # categories = ['IT', 'Science', 'Culture', 'Sport'] # Soroush\n",
    "        # TODO: uncomment yours\n",
    "        category_news = {}\n",
    "        for category in categories:\n",
    "            news = []\n",
    "            URLs = self.get_news_URLs_since(category, from_year, from_month)\n",
    "            with tqdm(URLs) as pbar:\n",
    "                pbar.set_description(f'[{category}] [Scraping news]')\n",
    "                for URL in pbar:\n",
    "                    news.append(self.parse_news(URL))\n",
    "            news = list(filter(None, news))\n",
    "            category_news[category] = news\n",
    "        return category_news\n",
    "\n",
    "scraper = Scraper(current_year=1401, current_month=3)\n",
    "category_news = scraper.scrape(from_year=1400, from_month=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for category, news in category_news.items():\n",
    "    df = pd.DataFrame(news)\n",
    "    df.to_csv(f\"{category}_dataset.csv\", encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}