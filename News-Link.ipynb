{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "l9k4nvPt6ttU"
   },
   "source": [
    "<div dir=\"auto\" align=\"center\">\n",
    "    <h3>\n",
    "        بسم الله الرحمن الرحیم\n",
    "    </h3>\n",
    "    <br>\n",
    "    <h1>\n",
    "        <strong>\n",
    "            بازیابی پیشرفته اطلاعات\n",
    "        </strong>\n",
    "    </h1>\n",
    "    <h2>\n",
    "        <strong>\n",
    "            تمرین پنجم (تحلیل لینک)\n",
    "        </strong>\n",
    "    </h2>\n",
    "    <br>\n",
    "    <h3>\n",
    "        محمد هجری - ٩٨١٠٦١٥٦\n",
    "        <br><br>\n",
    "        ارشان دلیلی - ٩٨١٠٥٧٥١\n",
    "        <br><br>\n",
    "        سروش جهان‌زاد - ٩٨١٠٠٣٨٩\n",
    "    </h3>\n",
    "    <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bDy59Vhk6ttY"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        دسترسی به داده‌ها و مدل‌های ذخیره شده\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        به دلیل حجم بالای فایل‌های ذخیره شده، از قرار دادن آن‌ها در پوشه نوتبوک صرف نظر کرده و با اجرای قطعه کد زیر، از طریق گوگل درایو به آن‌ها دسترسی پیدا می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXmeQEW66ttZ",
    "outputId": "b2b8b5b4-b661-4ede-9296-a44dbb63c663",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# address = \"University/Term 6/MIR/Homeworks/HW3\"\n",
    "# sys.path.append(f\"/content/drive/My Drive/{address}\")\n",
    "# %cd /content/drive/My\\ Drive/$address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Uhyo5mtc6tta"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        نصب و دسترسی به کتابخانه‌های مورد نیاز\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای دو قطعه کد زیر، کتابخانه‌هایی که از آن‌ها در این تمرین استفاده شده است، نصب و قابل استفاده می‌شوند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DJqkFOY6tta",
    "outputId": "6afc9c50-6b3c-4009-8537-1e940ae2be47",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install bs4\n",
    "! pip install tqdm\n",
    "! pip install pandas\n",
    "! pip install requests\n",
    "! pip install hazm\n",
    "! pip install unidecode\n",
    "! pip install datasets\n",
    "! pip install nltk\n",
    "! pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bs5P-S4T6ttb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import hazm\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "HlgX1wDm6ttb"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ١. دریافت داده‌ها\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این تمرین، بیش از ٦٨ هزار خبر از\n",
    "        <a href=\"https://www.hamshahrionline.ir/\"> وب‌سایت همشهری‌آنلاین </a>\n",
    "        گردآوری شده که در ١٠ دسته‌ی سیاسی، جهانی، اقتصادی، اجتماعی، شهری، ورزشی، علمی، فرهنگی، فناوری اطلاعات و مهارت‌های زندگی طبقه‌بندی شده‌اند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TFclx23L6ttc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'Politics': 'سیاسی',\n",
    "    'World': 'جهانی',\n",
    "    'Economy': 'اقتصادی',\n",
    "    'Society': 'اجتماعی',\n",
    "    'City': 'شهری',\n",
    "    'Sport': 'ورزشی',\n",
    "    'Science': 'علمی',\n",
    "    'Culture': 'فرهنگی',\n",
    "    'IT': 'فناوری اطلاعات',\n",
    "    'LifeSkills': 'مهارت‌های زندگی',\n",
    "}\n",
    "\n",
    "CATEGORIES_CLASSES = {\n",
    "    'Politics': 0,\n",
    "    'World': 1,\n",
    "    'Economy': 2,\n",
    "    'Society': 3,\n",
    "    'City': 4,\n",
    "    'Sport': 5,\n",
    "    'Science': 6,\n",
    "    'Culture': 7,\n",
    "    'IT': 8,\n",
    "    'LifeSkills': 9,\n",
    "}\n",
    "\n",
    "CLASSES_CATEGORIES = {v: k for k, v in CATEGORIES_CLASSES.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DgNCrnH56ttd"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        برای دریافت داده‌ها یک ماژول Scraper ساخته‌ایم که اخبار مربوط به ١٠ دسته‌ی مذکور را در بازه‌ی زمانی تعیین شده، کراول کرده و در فایل dataset.zip ذخیره و فشرده سازی می‌کند. کد مربوط به این ماژول را در زیر مشاهده می‌کنید.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fVKQF6R36ttd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self, current_year, current_month):\n",
    "        self.current_year = current_year\n",
    "        self.current_month = current_month\n",
    "\n",
    "    def get_URL_content(self, URL):\n",
    "        while True:\n",
    "            try:\n",
    "                return requests.get(URL, timeout=5).content\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def generate_page_URL(self, page_index, category, year, month):\n",
    "        tp = {'Politics': 6, 'World': 11, 'Economy': 10, 'Society': 5, 'City': 7,\n",
    "              'Sport': 9, 'Science': 20, 'Culture': 26, 'IT': 718, 'LifeSkills': 21}[category]\n",
    "        return f'https://www.hamshahrionline.ir/archive?pi={page_index}&tp={tp}&ty=1&ms=0&mn={month}&yr={year}'\n",
    "\n",
    "    def get_page_URLs_by_time(self, category, year, month):\n",
    "        URLs = []\n",
    "        page_index = 1\n",
    "        while True:\n",
    "            URL = self.generate_page_URL(page_index, category, year, month)\n",
    "            content = self.get_URL_content(URL)\n",
    "            if re.findall('pagination', str(content)):\n",
    "                URLs.append(URL)\n",
    "                page_index += 1\n",
    "            else:\n",
    "                break\n",
    "        return URLs\n",
    "\n",
    "    def get_page_URLs_since(self, category, year, month):\n",
    "        URLs = []\n",
    "        with tqdm() as pbar:\n",
    "            while True:\n",
    "                if month > 12:\n",
    "                    month = 1\n",
    "                    year += 1\n",
    "                pbar.set_description(f'[{category}] [Extracting page URLs] [Date: {year}/{month}]')\n",
    "                URLs_by_time = self.get_page_URLs_by_time(category, year, month)\n",
    "                if URLs_by_time:\n",
    "                    for URL in URLs_by_time:\n",
    "                        URLs.append(URL)\n",
    "                    month += 1\n",
    "                elif self.current_year > year or (self.current_year == year and self.current_month > month):\n",
    "                    month += 1\n",
    "                else:\n",
    "                    break\n",
    "        return URLs\n",
    "\n",
    "    def get_news_URLs_since(self, category, year, month):\n",
    "        news_URLs = []\n",
    "        page_URLs = self.get_page_URLs_since(category, year, month)\n",
    "        with tqdm(page_URLs) as pbar:\n",
    "            for page_URL in pbar:\n",
    "                content = self.get_URL_content(page_URL)\n",
    "                soup = BeautifulSoup(content, 'html5lib')\n",
    "                for item in soup.findAll('li', attrs={'class': 'news'}):\n",
    "                    URL = item.find('div', attrs={'class': 'desc'}).find('h3').find('a')['href']\n",
    "                    URL = 'https://www.hamshahrionline.ir' + URL\n",
    "                    news_URLs.append(URL)\n",
    "                pbar.set_description(f'[{category}] [Extracting news URLs] [{len(news_URLs)} news until now]')\n",
    "        return news_URLs\n",
    "\n",
    "    def parse_news(self, URL, category):\n",
    "        try:\n",
    "            content = self.get_URL_content(URL)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            date = soup.find('div', {'class': 'col-6 col-sm-4 col-xl-4 item-date'}).span.text.strip()\n",
    "            title = soup.find('div', {'class': 'item-title'}).h1.text.strip()\n",
    "            intro = soup.find('p', {'class': 'introtext', 'itemprop': 'description'}).text.strip()\n",
    "            body = soup.find('div', {'class': 'item-text', 'itemprop': 'articleBody'}).text.strip()\n",
    "            return {\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'intro': intro,\n",
    "                'body': body,\n",
    "                'category': category,\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def scrape(self, from_year, from_month):\n",
    "        categories = ['Politics', 'World', 'Economy', 'Society', 'City',\n",
    "                      'Sport', 'Science', 'Culture', 'IT', 'LifeSkills']\n",
    "        news = []\n",
    "        for category in categories:\n",
    "            URLs = self.get_news_URLs_since(category, from_year, from_month)\n",
    "            with tqdm(URLs) as pbar:\n",
    "                pbar.set_description(f'[{category}] [Scraping news]')\n",
    "                for URL in pbar:\n",
    "                    news.append(self.parse_news(URL, category))\n",
    "        news = list(filter(None, news))\n",
    "        pd.DataFrame(news).to_csv(f'dataset.csv', encoding='utf-8')\n",
    "        with zipfile.ZipFile('dataset.zip', 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.write('dataset.csv')\n",
    "        os.remove('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Cx3Sl-iS6tte"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Scraper ایجاد شده و شروع به دریافت و ذخیره‌سازی داده‌ها می‌کند. خبرهای دریافت شده همگی مربوط به قرن جدید، از سال ١٤٠٠ به بعد هستند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fc5LA2wh6ttf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scraper = Scraper(current_year=1401, current_month=3)\n",
    "# scraper.scrape(from_year=1400, from_month=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5kMts6qe6ttf"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        بعد از ذخیره شدن داده‌ها در فایل فشرده dataset.zip، آن‌ها را از این فایل استخراج کرده و وارد برنامه می‌کنیم. با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HuoWmM7P6ttf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset_from_file():\n",
    "    dataset = []\n",
    "    with zipfile.ZipFile('dataset.zip', 'r') as zip_file:\n",
    "        zip_file.extractall()\n",
    "    with open('dataset.csv', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            data = dict(zip(header[1:], row[1:]))\n",
    "            dataset.append(data)\n",
    "    os.remove('dataset.csv')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame(read_dataset_from_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SQOAtgLY6ttf"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "    با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "HIkDaiJC6ttg",
    "outputId": "4787e3e6-e43a-4431-8a10-93e340b8b2ce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               دسته  تعداد\n0          کل خبرها  68362\n1             سیاسی  15798\n2             جهانی   2895\n3           اقتصادی   8900\n4           اجتماعی  13585\n5              شهری   3853\n6             ورزشی   8348\n7              علمی   3190\n8            فرهنگی   6512\n9    فناوری اطلاعات    437\n10  مهارت‌های زندگی   4844",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>دسته</th>\n      <th>تعداد</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>کل خبرها</td>\n      <td>68362</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>سیاسی</td>\n      <td>15798</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>جهانی</td>\n      <td>2895</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>اقتصادی</td>\n      <td>8900</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اجتماعی</td>\n      <td>13585</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>شهری</td>\n      <td>3853</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ورزشی</td>\n      <td>8348</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>علمی</td>\n      <td>3190</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>فرهنگی</td>\n      <td>6512</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>فناوری اطلاعات</td>\n      <td>437</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>مهارت‌های زندگی</td>\n      <td>4844</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_dataset_info():\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    length_dict = {key: 0 for key in CATEGORIES.keys()}\n",
    "    for _, data in dataset.iterrows():\n",
    "        length_dict[data['category']] += 1\n",
    "\n",
    "    df_dict = {\n",
    "        'دسته': CATEGORIES.values(),\n",
    "        'تعداد': length_dict.values(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.index += 1\n",
    "    df.loc[0] = ['کل خبرها', len(dataset)]\n",
    "    df = df.sort_index()\n",
    "    display(df)\n",
    "\n",
    "\n",
    "display_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "YuwgMYpB6ttg"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ٢. پیش پردازش اولیه‌ی متن\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        ابزار مورد استفاده برای پیش‌پردازش متن ورودی به صورت ماژولار طراحی شده است؛ به طوری که با صدا زدن تابع preprocess از آن، متن داده شده با عبور از یک خط لوله به صورت مرحله به مرحله تغییر می‌کند تا به یک ساختار استاندارد برسد. این مراحل عبارتند از:\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        نرمال سازی داده‌ها (normalize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف لینک‌ها (remove_links)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف نشانه‌های نگارشی (remove_punctuations)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        واحد سازی داده‌ها (word_tokenize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف کلمات نامعتبر (remove_invalid_words)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف ایست‌واژه‌ها (remove_stopwords)\n",
    "    </li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "phcZ2s706ttg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, stopwords_path):\n",
    "        self.stopwords = []\n",
    "        with open(stopwords_path, encoding='utf-8') as file:\n",
    "            self.stopwords = file.read().split()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_links(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        words = self.word_tokenize(text)\n",
    "        words = self.remove_invalid_words(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    def normalize(self, text):\n",
    "        return hazm.Normalizer().normalize(text)\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        patterns = ['\\S*http\\S*', '\\S*www\\S*', '\\S+\\.ir\\S*', '\\S+\\.com\\S*', '\\S+\\.org\\S*', '\\S*@\\S*']\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        return re.sub(f'[{punctuation}؟،٪×÷»«]+', '', text)\n",
    "\n",
    "    def word_tokenize(self, text):\n",
    "        return hazm.word_tokenize(text)\n",
    "\n",
    "    def remove_invalid_words(self, words):\n",
    "        return [word for word in words if len(word) > 3 or re.match('^[\\u0600-\\u06FF]{2,3}$', word)]\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        return [word for word in words if word not in self.stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2tqXc9WR6tth",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_preprocessed_texts(texts, path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(texts, file)\n",
    "\n",
    "\n",
    "def load_preprocessed_texts(path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def data_to_text(data):\n",
    "    return ' '.join([data['title'], data['intro'], data['body']]).lower()\n",
    "\n",
    "\n",
    "def get_preprocessed_texts(dataset, preprocessor, mode, save=False):\n",
    "    preprocessed_texts = []\n",
    "    if mode == 'process':\n",
    "        texts = [data_to_text(data) for _, data in dataset.iterrows()]\n",
    "        preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    if mode == 'load':\n",
    "        preprocessed_texts = load_preprocessed_texts()\n",
    "    if save:\n",
    "        save_preprocessed_texts(preprocessed_texts)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "E3M4tMlL6tth"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Preprocessor ایجاد کرده و شروع به پیش پردازش داده‌ها می‌کنیم، یا داده‌های پیش‌پردازش‌شده‌ی ذخیره‌شده را از فایل مربوطه بازیابی می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Izni3XhQ6tth",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(stopwords_path='stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MYNCBkS8CTe",
    "outputId": "5e1acb32-2da0-4418-8ea2-223b3c9be2ae"
   },
   "outputs": [],
   "source": [
    "preprocessed_texts = get_preprocessed_texts(dataset, preprocessor, mode='load', save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Cmjunsz86tti"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "         با اجرای قطعه کد زیر، یک مجموعه داده‌ی کوچک‌تر از روی مجموعه داده‌ی اصلی ایجاد می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpxmLgeU6tti",
    "outputId": "0ae36064-f669-45d0-c9fe-92178b9c7139",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mini_dataset(len_each_category=1000):\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    mini_dataset = []\n",
    "    for category in CATEGORIES.keys():\n",
    "        dataset_by_category = dataset.loc[dataset['category'] == category]\n",
    "        length = min(len_each_category, dataset_by_category.shape[0])\n",
    "        mini_dataset.append(dataset_by_category.sample(length, random_state=1))\n",
    "\n",
    "    mini_dataset = pd.concat(mini_dataset).reset_index(drop=True)\n",
    "    return mini_dataset\n",
    "\n",
    "\n",
    "# Almost 10K News\n",
    "mini_dataset = get_mini_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7GgSPBt36tti"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ٣. رتبه‌بندی و تحلیل لینک\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "در این قسمت، با استفاده از دو روش PageRank و HITS به رتبه‌بندی اخبار می‌پردازیم. در قسمت اول ابتدا ماتریس مجاورت را برای اخبار دیتاست می‌سازیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ABtH0P3n6tti"
   },
   "source": [
    "<div>\n",
    "    <h4 style='direction:rtl;text-align:justify;'>\n",
    "       ۱. ساخت ماتریس مجاورت\n",
    "    </h4>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        برای ساخت ماتریس مجاورت به این صورت عمل می‌کنیم که هر خبر را به عنوان یک راس در نظر می‌گیریم. سپس با در نظر گرفتن تعداد کلمات مشترک هر دو خبر، اگر این تعداد از یک threshold مشخص بیش‌تر باشد، آن دو خبر را به یک‌دیگر وصل کرده و تعداد کلمات مشترک را به عنوان وزن آن یال دو طرفه لحاظ می‌کنیم. البته، در انتها، نرمال‌سازی نیز به صورت خودکار انجام خواهد شد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3eCMgSM_r2hK"
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 2\n",
    "\n",
    "def common_words(bow1, bow2):\n",
    "    bow1 = set(bow1)\n",
    "    bow2 = set(bow2)\n",
    "    same_words = bow1.intersection(bow2)\n",
    "    if len(same_words) >= THRESHOLD:\n",
    "        return len(same_words)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، عناوین خبری مجموعه‌ی داده‌ها را پیش‌پردازش کرده و به عنوان رئوس گراف در نظر می‌گیریم.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXdvbkMGr2hK",
    "outputId": "83bb597b-88ce-42a9-94a0-4b4e1a24bf9c"
   },
   "outputs": [],
   "source": [
    "def get_adj_matrix(dataset):\n",
    "    titles = dataset['title'].to_list()\n",
    "    titles = [preprocessor.preprocess(title) for title in titles]\n",
    "    adjacency_matrix = np.zeros(shape=(dataset.shape[0], dataset.shape[0]))\n",
    "    for i in tqdm(range(dataset.shape[0]), position=0, leave=True):\n",
    "        for j in range(i + 1, dataset.shape[0]):\n",
    "            weight = common_words(titles[i], titles[j])\n",
    "            adjacency_matrix[i, j] = weight\n",
    "            adjacency_matrix[j, i] = weight\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9437/9437 [01:35<00:00, 98.89it/s]  \n"
     ]
    }
   ],
   "source": [
    "adjacency_matrix = get_adj_matrix(mini_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، گراف مبتنی بر کتاب‌خانه‌ی networkx از روی ماتریس مجاورتی که در قبل به دست آمد، ساخته می‌شود.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "g4biARcOr2hL"
   },
   "outputs": [],
   "source": [
    "graph = nx.from_numpy_array(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "    <h4 style='direction:rtl;text-align:justify;'>\n",
    "       ٢. اجرای الگوریتم‌های PageRank و HITS\n",
    "    </h4>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_rankings(dataset, graph, method, k=5):\n",
    "    if method == 'PageRank':\n",
    "        pr = nx.pagerank(graph, alpha=0.9)\n",
    "        vals = np.array(list(pr.values()))\n",
    "        idx = np.argsort(-vals)\n",
    "        for i, title in enumerate(dataset.iloc[idx[:k]]['title']):\n",
    "            print(f\"Rank {i + 1} with pr = {pr[idx[i]]:.8f}\")\n",
    "            print(title)\n",
    "            print()\n",
    "    elif method == 'HITS':\n",
    "        h, a = nx.hits(graph)\n",
    "        vals_h = np.array(list(h.values()))\n",
    "        idx_h = np.argsort(-vals_h)\n",
    "        vals_a = np.array(list(a.values()))\n",
    "        idx_a = np.argsort(-vals_a)\n",
    "        print(\"---------------------------------------- Hubs ----------------------------------------\")\n",
    "        for i, title in enumerate(dataset.iloc[idx_h[:k]]['title']):\n",
    "            print(f\"Rank {i + 1} with hub value = {h[idx_h[i]]:.8f}\")\n",
    "            print(title)\n",
    "            print()\n",
    "        print()\n",
    "        print(\"------------------------------------- Authorities ------------------------------------\")\n",
    "        for i, title in enumerate(dataset.iloc[idx_a[:k]]['title']):\n",
    "            print(f\"Rank {i + 1} with authority value = {a[idx_a[i]]:.8f}\")\n",
    "            print(title)\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Not valid method!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 with pr = 0.00106249\n",
      "ببینید | اوکراین؛ از آژیر حمله هوایی تا آتش سوزی در فرودگاه | روسیه: این جنگ نیست! | زلنسکی وضعیت نظامی اعلام کرد | واکنش آمریکا، انگلیس و ترکیه\n",
      "\n",
      "Rank 2 with pr = 0.00094111\n",
      "فردا ۱/۵ میلیون دوز واکسن کرونا به ایران می‌رسد | جزئیات واکسیناسیون گروه‌های سنی در جنوب و شرق کشور\n",
      "\n",
      "Rank 3 with pr = 0.00085860\n",
      "آمار تفکیکی واکسیناسیون کرونا در ایران تا ۲۳ شهریور | ۱۲ میلیون نفر هردو دوز واکسن را زدند\n",
      "\n",
      "Rank 4 with pr = 0.00085610\n",
      "۵ واکسن ایرانی کرونا به مرحله نهایی رسیدند | نیاز به ۱۲۰ میلیون دوز برای واکسیناسیون ۷۰ درصد جمعیت ایران\n",
      "\n",
      "Rank 5 with pr = 0.00084152\n",
      "زمان ارسال لایحه بودجه ۱۴۰۱ شهرداری تهران به شورای شهر اعلام شد | قوانین و مقررات شهرسازی دست و پای مردم را بسته است\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_top_rankings(mini_dataset, graph, method='PageRank')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "    <h4 style='direction:rtl;text-align:justify;'>\n",
    "       تحلیل خروجی الگوریتم PageRank\n",
    "    </h4>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در اولین نگاه، نکته‌ای که توجه‌مان را جلب می‌کند تعداد کلمه‌ی عنوان محوری‎ترین خبرها است. این را انتظار داشتیم؛ چرا که با توجه به این که وزن یال‌های ورودی و خروجی یک راس متناسب با تعداد کلمات مشترک عنوان خبر آن راس با راس دیگر است، عناوین طولانی‌تر شانس بیش‌تری برای داشتن اشتراک با عناوین دیگر دارند. حال، کمی دقیق‌تر نگاه کنیم:\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        مهم‌ترین خبری که رنک اول را دارد، ویژگی‌های جالبی دارد. اسامی پنج کشور اوکراین، روسیه، آمریکا، انگلیس و ترکیه در آن وجود دارد. به وضوح می‌توان حدس زد که یکی از عوامل مهم بودن این خبر همین است! چرا که در تمام دسته‌های مختلف این امکان هست که راجع به این کشورهای مهم صحبت شود؛ علی الخصوص کشورهای اوکراین، روسیه و آمریکا که تعداد بسیاری از خبرهای چند ماه گذشته را به خود اختصاص می‌دهند. نکته‌ی دیگر این است که با توجه به به‌روز بودن مجموعه‌ی داده‌های ما، خبرهای مربوط به جنگ‌های اخیر در آن زیاد است؛ پس، انتظار داریم کلمات کلیدی‌ای مانند \"جنگ\"، \"نظامی\"، \"حمله\" و حتی \"آتش\" بر روی ایجاد ارتباط این خبر با خبرهای دیگر موثر باشد.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        هم‌چنین، می‌توان دید که مهم‌ترین خبرهای بعدی‌ مربوط به کرونا و واکسیناسون هستند. این نیز بسیار مورد انتظار است. توجه داشته باشید که خبرهای کراول شده‌ی ما همگی پس از دوره کرونا هستند و داغ‌‌ترین خبرهای مربوط به کرونا نیز مربوط به واکسیناسیون انجام شده داخل ایران می‌باشند. همین کلمات کلیدی‌ و پرتکرار \"کرونا\"، \"واکسن\"، \"واکسیناسیون\"، \"دوز\" و حتی خود \"ایران\" می‌توانند روی رنک این خبرها اثر به‌سزایی داشته باشند.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        مشابه مواردی که گفته شد، کلمات مربوط به تهران و شهرداری آن نیز در خبرهای ما بسیار است. علی الخصوص که یک دسته با عنوان \"City\" شامل همین جنس خبرها است. به همین دلیل دور از انتظار نیست که رنک پنجم مربوط به یک خبر مهم از همین دسته باشد.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        توجه کنید که اختلاف بین pr خبرهای رنک با یک‌دیگر بسیار کم – چیزی در حدود یک‌ صدهزارم – است. پس، ممکن است با توجه به این اختلاف ناچیز تعداد دیگری از خبرهای مهم در رنک‌های بعدی قرار گرفته باشند و این با توجه به ماهیت PageRank یک امر طبیعی است.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "vzMZOO6lr2hM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\networkx\\algorithms\\link_analysis\\hits_alg.py:78: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Hubs ----------------------------------------\n",
      "Rank 1 with hub value = 0.00812863\n",
      "نرخ یورو و  ۲۸ ارز دیگر افزایش یافت | جدیدترین قیمت رسمی ارزها در ۹ مرداد ۱۴۰۰\n",
      "\n",
      "Rank 2 with hub value = 0.00798444\n",
      "قیمت رسمی دلار به کانال ۲۷ هزار تومان نزدیک شد | جدیدترین قیمت ارزها در یکم شهریور ۱۴۰۰\n",
      "\n",
      "Rank 3 with hub value = 0.00797153\n",
      "کاهش قیمت طلا و سکه در بازار | جدیدترین نرخ طلا و سکه در ۲۲ خرداد ۱۴۰۰\n",
      "\n",
      "Rank 4 with hub value = 0.00796374\n",
      "نرخ ۲۰ ارز افزایش یافت | جدیدترین قیمت رسمی ارزها در ۱۲ خرداد ۱۴۰۰\n",
      "\n",
      "Rank 5 with hub value = 0.00785182\n",
      "نرخ ۲۸ ارز کاهش یافت | جدیدترین قیمت رسمی ارزها در ۱۷شهریور ۱۴۰۰\n",
      "\n",
      "\n",
      "------------------------------------- Authorities ------------------------------------\n",
      "Rank 1 with authority value = 0.00812863\n",
      "نرخ یورو و  ۲۸ ارز دیگر افزایش یافت | جدیدترین قیمت رسمی ارزها در ۹ مرداد ۱۴۰۰\n",
      "\n",
      "Rank 2 with authority value = 0.00798444\n",
      "قیمت رسمی دلار به کانال ۲۷ هزار تومان نزدیک شد | جدیدترین قیمت ارزها در یکم شهریور ۱۴۰۰\n",
      "\n",
      "Rank 3 with authority value = 0.00797153\n",
      "کاهش قیمت طلا و سکه در بازار | جدیدترین نرخ طلا و سکه در ۲۲ خرداد ۱۴۰۰\n",
      "\n",
      "Rank 4 with authority value = 0.00796374\n",
      "نرخ ۲۰ ارز افزایش یافت | جدیدترین قیمت رسمی ارزها در ۱۲ خرداد ۱۴۰۰\n",
      "\n",
      "Rank 5 with authority value = 0.00785182\n",
      "نرخ ۲۸ ارز کاهش یافت | جدیدترین قیمت رسمی ارزها در ۱۷شهریور ۱۴۰۰\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_top_rankings(mini_dataset, graph, method='HITS')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "    <h4 style='direction:rtl;text-align:justify;'>\n",
    "       تحلیل خروجی الگوریتم HITS\n",
    "    </h4>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با توجه به مجموعه‌ی داده‌های ما و نحوه‌ی عملکرد روش HITS این خروجی به شدت انتظار می‌رفت. به قول یک بزرگی، اقتصاد همه چیز است!\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در خبرهای کراول شده‌ی ما بخش حائز اهمیتی را خبرهای اقتصادی تشکیل می‌دهند و از این بخش قابل توجه، اکثر اخبار مربوط به نوسانات و رویدادهای مربوط به بازار ارزها، طلا و سکه است. خصوصاً که یک سری خبر روزانه در این مورد منتشر می‌شود که امور اقتصادی مربوط به این‌ها را گزارش می‌دهد. این جنس خبرها که در خروجی آمده‌اند، بسیار به یک دیگر شبیه هستند؛ حتی قالب نوشتاری آن‌ها نیز بسیار مشابه یک‌دیگر است. بنابراین در گراف ارتباطات خبری ما، یک زیرگراف چگال مربوط به این خبرها داریم که تشکیل یک زیرگراف حدوداً کامل را داده‌اند؛ مثل یک مهمانی که در آن همه‌ی مهمان‌ها به یک‌دیگر دست می‌دهند! به همین دلیل است که هم در بخش Hub و هم در بخش Authority خبرهای حدوداً یکسانی را می‌بینیم؛ این خبرها به یک‌دیگر ارجاع می‌دهند و می‌گیرند.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        شاید این سوال پیش بیاید که چرا در بخش PageRank این گونه خبرها را ندیدیم؟ پاسخ بسیار ساده‌ای برای این سوال وجود دارد. توجه کنید که در الگوریتم PageRank همواره مجموع وزن نهایی یال‌های خروجی از هر راس برابر با یک است؛ به عبارت دیگر، نرمال‌سازی وزن‌ها را داریم. این در حالی است که در روش HITS به تعداد یال‌های خروجی از هر راس اهمیت ویژه‌ای می‌دهیم. برای خبرهای اقتصادی مذکور، این موضوع به روشنی دیده می‌شود، ولی به دلیل نرمال‌سازی در روش PageRank این ویژگی کم‌اثر خواهد بود.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        از این که وقت ارزشمند خود را برای بررسی این نوتبوک صرف کرده‌اید، صمیمانه سپاسگزاریم. (:\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "News-Link.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}