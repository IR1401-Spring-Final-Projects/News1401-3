{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "0ukP3ituDSCG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"center\">\n",
    "    <h3>\n",
    "        بسم الله الرحمن الرحیم\n",
    "    </h3>\n",
    "    <br>\n",
    "    <h1>\n",
    "        <strong>\n",
    "            بازیابی پیشرفته اطلاعات\n",
    "        </strong>\n",
    "    </h1>\n",
    "    <h2>\n",
    "        <strong>\n",
    "            تمرین سوم (موتور جستجوی اخبار)\n",
    "        </strong>\n",
    "    </h2>\n",
    "    <br>\n",
    "    <h3>\n",
    "        محمد هجری - ٩٨١٠٦١٥٦\n",
    "        <br><br>\n",
    "        ارشان دلیلی - ٩٨١٠٥٧٥١\n",
    "        <br><br>\n",
    "        سروش جهان‌زاد - ٩٨١٠٠٣٨٩\n",
    "    </h3>\n",
    "    <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cEdo5ea3j39X"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        دسترسی به داده‌ها و مدل‌های ذخیره شده\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        به دلیل حجم بالای فایل‌های ذخیره شده، از قرار دادن آن‌ها در پوشه نوتبوک صرف نظر کرده و با اجرای قطعه کد زیر، از طریق گوگل درایو به آن‌ها دسترسی پیدا می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vcuDlf6j39Y",
    "outputId": "0e379dab-0d3a-4d06-c1c8-6710a4d0277c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/University/Term 6/MIR/Homeworks/HW3\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# address = \"University/Term 6/MIR/Homeworks/HW3\"\n",
    "# sys.path.append(f\"/content/drive/My Drive/{address}\")\n",
    "# %cd /content/drive/My\\ Drive/$address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IR0F_wAbDSCJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        نصب و دسترسی به کتابخانه‌های مورد نیاز\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای دو قطعه کد زیر، کتابخانه‌هایی که از آن‌ها در این تمرین استفاده شده است، نصب و قابل استفاده می‌شوند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfxCzBqoDSCK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install hazm\n",
    "!pip install unidecode\n",
    "!pip install fasttext\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sIglKdxHDSCM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import hazm\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aNJYaFC1DSCM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ١. دریافت داده‌ها\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این تمرین، بیش از ٦٨ هزار خبر از\n",
    "        <a href=\"https://www.hamshahrionline.ir/\"> وب‌سایت همشهری‌آنلاین </a>\n",
    "        گردآوری شده که در ١٠ دسته‌ی سیاسی، جهانی، اقتصادی، اجتماعی، شهری، ورزشی، علمی، فرهنگی، فناوری اطلاعات و مهارت‌های زندگی طبقه‌بندی شده‌اند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I5D13VU7DSCN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'Politics': 'سیاسی',\n",
    "    'World': 'جهانی',\n",
    "    'Economy': 'اقتصادی',\n",
    "    'Society': 'اجتماعی',\n",
    "    'City': 'شهری',\n",
    "    'Sport': 'ورزشی',\n",
    "    'Science': 'علمی',\n",
    "    'Culture': 'فرهنگی',\n",
    "    'IT': 'فناوری اطلاعات',\n",
    "    'LifeSkills': 'مهارت‌های زندگی',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gviMdkLQDSCN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        برای دریافت داده‌ها یک ماژول Scraper ساخته‌ایم که اخبار مربوط به ١٠ دسته‌ی مذکور را در بازه‌ی زمانی تعیین شده، کراول کرده و در فایل dataset.zip ذخیره و فشرده سازی می‌کند. کد مربوط به این ماژول را در زیر مشاهده می‌کنید.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zlmFrCEHDSCO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self, current_year, current_month):\n",
    "        self.current_year = current_year\n",
    "        self.current_month = current_month\n",
    "\n",
    "    def get_URL_content(self, URL):\n",
    "        while True:\n",
    "            try:\n",
    "                return requests.get(URL, timeout=5).content\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def generate_page_URL(self, page_index, category, year, month):\n",
    "        tp = {'Politics': 6, 'World': 11, 'Economy': 10, 'Society': 5, 'City': 7,\n",
    "              'Sport': 9, 'Science': 20, 'Culture': 26, 'IT': 718, 'LifeSkills': 21}[category]\n",
    "        return f'https://www.hamshahrionline.ir/archive?pi={page_index}&tp={tp}&ty=1&ms=0&mn={month}&yr={year}'\n",
    "\n",
    "    def get_page_URLs_by_time(self, category, year, month):\n",
    "        URLs = []\n",
    "        page_index = 1\n",
    "        while True:\n",
    "            URL = self.generate_page_URL(page_index, category, year, month)\n",
    "            content = self.get_URL_content(URL)\n",
    "            if re.findall('pagination', str(content)):\n",
    "                URLs.append(URL)\n",
    "                page_index += 1\n",
    "            else:\n",
    "                break\n",
    "        return URLs\n",
    "\n",
    "    def get_page_URLs_since(self, category, year, month):\n",
    "        URLs = []\n",
    "        with tqdm() as pbar:\n",
    "            while True:\n",
    "                if month > 12:\n",
    "                    month = 1\n",
    "                    year += 1\n",
    "                pbar.set_description(f'[{category}] [Extracting page URLs] [Date: {year}/{month}]')\n",
    "                URLs_by_time = self.get_page_URLs_by_time(category, year, month)\n",
    "                if URLs_by_time:\n",
    "                    for URL in URLs_by_time:\n",
    "                        URLs.append(URL)\n",
    "                    month += 1\n",
    "                elif self.current_year > year or (self.current_year == year and self.current_month > month):\n",
    "                    month += 1\n",
    "                else:\n",
    "                    break\n",
    "        return URLs\n",
    "\n",
    "    def get_news_URLs_since(self, category, year, month):\n",
    "        news_URLs = []\n",
    "        page_URLs = self.get_page_URLs_since(category, year, month)\n",
    "        with tqdm(page_URLs) as pbar:\n",
    "            for page_URL in pbar:\n",
    "                content = self.get_URL_content(page_URL)\n",
    "                soup = BeautifulSoup(content, 'html5lib')\n",
    "                for item in soup.findAll('li', attrs={'class': 'news'}):\n",
    "                    URL = item.find('div', attrs={'class': 'desc'}).find('h3').find('a')['href']\n",
    "                    URL = 'https://www.hamshahrionline.ir' + URL\n",
    "                    news_URLs.append(URL)\n",
    "                pbar.set_description(f'[{category}] [Extracting news URLs] [{len(news_URLs)} news until now]')\n",
    "        return news_URLs\n",
    "\n",
    "    def parse_news(self, URL, category):\n",
    "        try:\n",
    "            content = self.get_URL_content(URL)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            date = soup.find('div', {'class': 'col-6 col-sm-4 col-xl-4 item-date'}).span.text.strip()\n",
    "            title = soup.find('div', {'class': 'item-title'}).h1.text.strip()\n",
    "            intro = soup.find('p', {'class': 'introtext', 'itemprop': 'description'}).text.strip()\n",
    "            body = soup.find('div', {'class': 'item-text', 'itemprop': 'articleBody'}).text.strip()\n",
    "            return {\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'intro': intro,\n",
    "                'body': body,\n",
    "                'category': category,\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def scrape(self, from_year, from_month):\n",
    "        categories = ['Politics', 'World', 'Economy', 'Society', 'City',\n",
    "                      'Sport', 'Science', 'Culture', 'IT', 'LifeSkills']\n",
    "        news = []\n",
    "        for category in categories:\n",
    "            URLs = self.get_news_URLs_since(category, from_year, from_month)\n",
    "            with tqdm(URLs) as pbar:\n",
    "                pbar.set_description(f'[{category}] [Scraping news]')\n",
    "                for URL in pbar:\n",
    "                    news.append(self.parse_news(URL, category))\n",
    "        news = list(filter(None, news))\n",
    "        pd.DataFrame(news).to_csv(f'dataset.csv', encoding='utf-8')\n",
    "        with zipfile.ZipFile('dataset.zip', 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.write('dataset.csv')\n",
    "        os.remove('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Kt-SS3ReDSCP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Scraper ایجاد شده و شروع به دریافت و ذخیره‌سازی داده‌ها می‌کند. خبرهای دریافت شده همگی مربوط به قرن جدید، از سال ١٤٠٠ به بعد هستند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uiwTDzE6DSCP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scraper = Scraper(current_year=1401, current_month=3)\n",
    "#scraper.scrape(from_year=1400, from_month=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2ynwZYeLDSCQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        بعد از ذخیره شدن داده‌ها در فایل فشرده dataset.zip، آن‌ها را از این فایل استخراج کرده و وارد برنامه می‌کنیم. با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bFUFTo7mDSCQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset_from_file():\n",
    "    dataset = []\n",
    "    with zipfile.ZipFile('dataset.zip', 'r') as zip_file:\n",
    "        zip_file.extractall()\n",
    "    with open('dataset.csv', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            data = dict(zip(header[1:], row[1:]))\n",
    "            dataset.append(data)\n",
    "    os.remove('dataset.csv')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame(read_dataset_from_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OkLqdbKkDSCR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "    با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "ktN_vJjlDSCR",
    "outputId": "edf2e129-d901-4de8-9b56-2383578c4585",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>دسته</th>\n",
       "      <th>تعداد</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>کل خبرها</td>\n",
       "      <td>68362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سیاسی</td>\n",
       "      <td>15798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>جهانی</td>\n",
       "      <td>2895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اقتصادی</td>\n",
       "      <td>8900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اجتماعی</td>\n",
       "      <td>13585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>شهری</td>\n",
       "      <td>3853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ورزشی</td>\n",
       "      <td>8348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>علمی</td>\n",
       "      <td>3190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>فرهنگی</td>\n",
       "      <td>6512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>فناوری اطلاعات</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>مهارت‌های زندگی</td>\n",
       "      <td>4844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               دسته  تعداد\n",
       "0          کل خبرها  68362\n",
       "1             سیاسی  15798\n",
       "2             جهانی   2895\n",
       "3           اقتصادی   8900\n",
       "4           اجتماعی  13585\n",
       "5              شهری   3853\n",
       "6             ورزشی   8348\n",
       "7              علمی   3190\n",
       "8            فرهنگی   6512\n",
       "9    فناوری اطلاعات    437\n",
       "10  مهارت‌های زندگی   4844"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_dataset_info():\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    length_dict = {key: 0 for key in CATEGORIES.keys()}\n",
    "    for _, data in dataset.iterrows():\n",
    "        length_dict[data['category']] += 1\n",
    "\n",
    "    df_dict = {\n",
    "        'دسته': CATEGORIES.values(),\n",
    "        'تعداد': length_dict.values(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.index += 1\n",
    "    df.loc[0] = ['کل خبرها', len(dataset)]\n",
    "    df = df.sort_index()\n",
    "    display(df)\n",
    "\n",
    "\n",
    "display_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Bah1P3lNDSCS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ٢. پیش پردازش اولیه‌ی متن\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        ابزار مورد استفاده برای پیش‌پردازش متن ورودی به صورت ماژولار طراحی شده است؛ به طوری که با صدا زدن تابع preprocess از آن، متن داده شده با عبور از یک خط لوله به صورت مرحله به مرحله تغییر می‌کند تا به یک ساختار استاندارد برسد. این مراحل عبارتند از:\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        نرمال سازی داده‌ها (normalize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف لینک‌ها (remove_links)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف نشانه‌های نگارشی (remove_punctuations)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        واحد سازی داده‌ها (word_tokenize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف کلمات نامعتبر (remove_invalid_words)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف ایست‌واژه‌ها (remove_stopwords)\n",
    "    </li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SjtCqkWsDSCS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, stopwords_path):\n",
    "        self.stopwords = []\n",
    "        with open(stopwords_path, encoding='utf-8') as file:\n",
    "            self.stopwords = file.read().split()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_links(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        words = self.word_tokenize(text)\n",
    "        words = self.remove_invalid_words(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    def normalize(self, text):\n",
    "        return hazm.Normalizer().normalize(text)\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        patterns = ['\\S*http\\S*', '\\S*www\\S*', '\\S+\\.ir\\S*', '\\S+\\.com\\S*', '\\S+\\.org\\S*', '\\S*@\\S*']\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        return re.sub(f'[{punctuation}؟،٪×÷»«]+', '', text)\n",
    "\n",
    "    def word_tokenize(self, text):\n",
    "        return hazm.word_tokenize(text)\n",
    "\n",
    "    def remove_invalid_words(self, words):\n",
    "        return [word for word in words if len(word) > 3 or re.match('^[\\u0600-\\u06FF]{2,3}$', word)]\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        return [word for word in words if word not in self.stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BjH18UKt0b9O"
   },
   "outputs": [],
   "source": [
    "def save_preprocessed_texts(texts, path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(texts, file)\n",
    "\n",
    "\n",
    "def load_preprocessed_texts(path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def data_to_text(data):\n",
    "    return ' '.join([data['title'], data['intro'], data['body']]).lower()\n",
    "\n",
    "\n",
    "def get_preprocessed_texts(dataset, preprocessor, mode, save=False):\n",
    "    preprocessed_texts = []\n",
    "    if mode == 'process':\n",
    "        texts = [data_to_text(data) for _, data in dataset.iterrows()]\n",
    "        preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    if mode == 'load':\n",
    "        preprocessed_texts = load_preprocessed_texts()\n",
    "    if save:\n",
    "        save_preprocessed_texts(preprocessed_texts)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SQqYXKb1DSCT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Preprocessor ایجاد کرده و شروع به پیش پردازش داده‌ها می‌کنیم، یا داده‌های پیش‌پردازش‌شده‌ی ذخیره‌شده را از فایل مربوطه بازیابی می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8cc5G2zWDSCT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(stopwords_path='stopwords.txt')\n",
    "preprocessed_texts = get_preprocessed_texts(dataset, preprocessor, mode='load', save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2ZLEketNj39j",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با توجه به این که حجم مجموعه داده‌ها باعث ایجاد محدودیت حافظه و زمان برای بعضی از مدل‌ها می‌شود، ناچار هستیم کسری از مجموعه‌ی داده‌ها را به عنوان ورودی برای یادگیری به آن‌ها بدهیم. با اجرای قطعه کد زیر، یک مجموعه داده‌ی کوچک‌تر از روی مجموعه داده‌ی اصلی ایجاد می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKMAe5bwDSCU",
    "outputId": "2fc00f8a-3e97-4bf2-b4dd-fe4372533e75",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:25<00:00, 153.97it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_mini_dataset(len_each_category=400):\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    mini_dataset = []\n",
    "    for category in CATEGORIES.keys():\n",
    "        dataset_by_category = dataset.loc[dataset['category'] == category]\n",
    "        length = min(len_each_category, dataset_by_category.shape[0])\n",
    "        mini_dataset.append(dataset_by_category.sample(length, random_state=1))\n",
    "\n",
    "    mini_dataset = pd.concat(mini_dataset).reset_index(drop=True)\n",
    "    texts = [data_to_text(data) for _, data in mini_dataset.iterrows()]\n",
    "    mini_preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    return mini_dataset, mini_preprocessed_texts\n",
    "\n",
    "\n",
    "mini_dataset, mini_preprocessed_texts = get_mini_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdQgh_S1DSCT"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ٣. دریافت اخبار مرتبط با کوئری کاربر\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این مرحله با استفاده از چهار مدل متفاوت زبانی، شروع به یادگیری روی خبرهای استخراج شده می‌کنیم و در انتها، نتیجه اجرای ده کوئری را بر روی هر یک مشاهده می‌کنیم. کوئری‌های مورد استفاده در این بخش در زیر آمده‌اند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ji8WjLVOuYpT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "QUERIES = [\n",
    "    \"نتیجه توافق برجام\",\n",
    "    \"حمله ارتش روسیه به اوکراین\",\n",
    "    \"افزایش نرخ تورم در کشور\",\n",
    "    \"آمار فوتی‌های کرونا\",\n",
    "    \"اقدامات شهرداری تهران\",\n",
    "    \"صعود ایران به جام جهانی\",\n",
    "    \"بیماری آبله میمونی\",\n",
    "    \"جشنواره فیلم فجر\",\n",
    "    \"رشد رمزارزها\",\n",
    "    \"روش پخت غذا\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emhsgEWHDSCY"
   },
   "source": [
    "<div>\n",
    "    <h4 style='direction:rtl;text-align:justify;'>\n",
    "        ٤. مدل مبتنی بر میانگین وزن‌دار بردارهای تعبیه، مانند FastText\n",
    "    </h4>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این بخش، از کتاب‌خانه‌ی fasttext استفاده می‌کنیم و با استفاده از روش skipgram و یک فایل شامل داده‌های موجود اخبار، مدل را به صورت unsupervised به یادگیری وامی‌داریم. پارامترهای استفاده شده‌ی دیگر هم در کد مشخصند؛ به طور مثال، کمینه و بیشینه n را برابر ۲ و ۵ در نظر گرفته‌ایم. پس از انجام یادگیری، میانگین امبدینگ‌ها را در یک آرایه‌ی نامپای ذخیره می‌کنیم. تمام این کارها با صدا زدن متد prepare با حالت train\n",
    "        رخ می‌دهند. همچنین می‌توانیم این متد را با حالت load صدا بزنیم تا یک مدل ذخیره شده را از فایل بازیابی کنیم.\n",
    "    </p>\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        علاوه بر این، می‌توانیم با استفاده از متدهای save_FastText_model و load_FastText_model این مدل و جزئیاتش را ذخیره یا بازیابی کنیم. البته اگر بخواهیم عمل ذخیره را بلافاصله پس از پایان یادگیری انجام دهیم هم می‌توانیم مقدار True را به آرگومان save در متد prepare بدهیم.\n",
    "    </p>\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        پس از پایان یادگیری یا بازیابی مدل یادگرفته شده از فایل ذخیره شده، می‌توانیم با استفاده از متد predict به تعداد دلخواه خبر مرتبط به درخواستمان را به دست بیاوریم. برای این کار، مدلمان بردارهای مربوط به درخواست را به دست می‌آورد و با محاسبه‌ی کسینوس‌ها و به دست آوردن میزان شباهت بین درخواست و خبرهای موجود، شبیه‌ترین خبرها را خروجی می‌دهد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6coHV2SJDSCZ"
   },
   "outputs": [],
   "source": [
    "class FastText:\n",
    "\n",
    "    def __init__(self, preprocessor=None, method='skipgram'):\n",
    "        self.method = method\n",
    "        self.mean_embed = []\n",
    "        self.model = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def train(self, texts):\n",
    "        with open('FastText_train.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(list(map(lambda doc: ' '.join(doc), texts))))\n",
    "        self.model = fasttext.train_unsupervised('FastText_train.txt', self.method, minn=2, maxn=5, wordNgrams=10)\n",
    "        os.remove('FastText_train.txt')\n",
    "        self.mean_embed = list(map(lambda doc:\n",
    "                                   np.mean(list(map(lambda word:\n",
    "                                                    self.model.get_word_vector(word), doc)), axis=0), texts))\n",
    "        self.mean_embed = np.array(self.mean_embed)\n",
    "\n",
    "    def predict(self, query, dataset, k):\n",
    "        if self.preprocessor:\n",
    "            query = self.preprocessor.preprocess(query)\n",
    "        if type(query) == str:\n",
    "            query = query.split()\n",
    "        query_embed = np.mean(list(map(lambda word: self.model.get_word_vector(word), query)), axis=0)\n",
    "        dataset_sim = np.array(list(map(lambda doc: self.cosine_sim(query_embed, doc), self.mean_embed)))\n",
    "        idx = np.argsort(-dataset_sim)\n",
    "        return dataset.iloc[list(idx[:k])]\n",
    "\n",
    "    def expand_query(self, query, k=5, lambda_0=1, lambda_1=1):\n",
    "        if self.preprocessor:\n",
    "            query = self.preprocessor.preprocess(query)\n",
    "        if type(query) == str:\n",
    "            query = query.split()\n",
    "        query_embed = np.mean(list(map(lambda word: self.model.get_word_vector(word), query)), axis=0)\n",
    "        dataset_sim = np.array(list(map(lambda doc: self.cosine_sim(query_embed, doc), self.mean_embed)))\n",
    "        idx = np.argsort(-dataset_sim)\n",
    "        relevant_docs_mean = np.mean(self.mean_embed[list(idx[:k]), :], axis=0)\n",
    "        irrelevant_docs_mean = np.mean(self.mean_embed[list(idx[-k:]), :], axis=0)\n",
    "        final_embed = query_embed + lambda_0 * relevant_docs_mean - lambda_1 * irrelevant_docs_mean\n",
    "        return final_embed\n",
    "    \n",
    "    def predict_query_vector(self, query_embed, dataset, k):\n",
    "        dataset_sim = np.array(list(map(lambda doc: self.cosine_sim(query_embed, doc), self.mean_embed)))\n",
    "        idx = np.argsort(-dataset_sim)\n",
    "        return dataset.iloc[list(idx[:k])]\n",
    "\n",
    "    def cosine_sim(self, query, doc):\n",
    "        return np.dot(query, doc) / (np.linalg.norm(query) * np.linalg.norm(doc))\n",
    "\n",
    "    def save_FastText_model(self, path='FastText_model.bin'):\n",
    "        self.model.save_model(path)\n",
    "        np.save('FastText_mean_embed.npy', self.mean_embed)\n",
    "\n",
    "    def load_FastText_model(self, path=\"FastText_model.bin\"):\n",
    "        self.model = fasttext.load_model(path)\n",
    "        self.mean_embed = np.load('FastText_mean_embed.npy')\n",
    "\n",
    "    def prepare(self, dataset, mode, save=False):\n",
    "        if mode == 'train':\n",
    "            self.train(dataset)\n",
    "        if mode == 'load':\n",
    "            self.load_FastText_model()\n",
    "        if save:\n",
    "            self.save_FastText_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "w12YOYKdj39x"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از مدل FastText ایجاد کرده و شروع به یادگیری داده‌ها و یا خواندن مدل ذخیره شده از روی فایل می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rLxjzX57DSCa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "FastText_model = FastText()\n",
    "FastText_model.prepare(preprocessed_texts, mode='load', save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96251285\n"
     ]
    }
   ],
   "source": [
    "qu = FastText_model.expand_query('گوشی')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63420</th>\n",
       "      <td>شنبه ۳۱ اردیبهشت ۱۴۰۱ - ۱۰:۴۲</td>\n",
       "      <td>معرفی بهترین گوشی تا قیمت ۸ میلیون تومان</td>\n",
       "      <td>شما می توانید با بررسی معروف ترین و بهترین گوش...</td>\n",
       "      <td>از سوی دیگر در سال های اخیر قیمت مربوط به گوشی...</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23850</th>\n",
       "      <td>چهارشنبه ۲۴ آذر ۱۴۰۰ - ۰۸:۵۴</td>\n",
       "      <td>جدول | جدیدترین قیمت گوشی اپل در بازار | حداقل...</td>\n",
       "      <td>در این گزارش قیمت انواع گوشی اپل موجود در بازا...</td>\n",
       "      <td>به گزارش همشهری آنلاین به نقل از باشگاه خبرنگا...</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23876</th>\n",
       "      <td>سه‌شنبه ۲۳ آذر ۱۴۰۰ - ۰۹:۵۶</td>\n",
       "      <td>جدول | جدیدترین قیمت انواع گوشی‌ | ارزان‌ترین ...</td>\n",
       "      <td>در این گزارش قیمت انواع گوشی شیائومی در بازار ...</td>\n",
       "      <td>به گزارش همشهری آنلاین به نقل از باشگاه خبرنگا...</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26393</th>\n",
       "      <td>جمعه ۲۶ فروردین ۱۴۰۱ - ۰۸:۱۹</td>\n",
       "      <td>قیمت‌های عجیب انواع گوشی آیفون | موبایل ۶۰ میل...</td>\n",
       "      <td>در بازار موبایل انواع گوشی‌های آیفون ۱۳ و ۱۲ ر...</td>\n",
       "      <td>به گزارش همشهری آنلاین به نقل از خبرآنلاین، در...</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23971</th>\n",
       "      <td>پنجشنبه ۱۸ آذر ۱۴۰۰ - ۰۹:۲۷</td>\n",
       "      <td>جدول | جدیدترین قیمت انواع گوشی‌ | معرفی گران‌...</td>\n",
       "      <td>پرچمدار گران‌ترین گوشی‌ در بازار موبایل گوشی س...</td>\n",
       "      <td>به گزارش همشهری آنلاین به نقل از خبرآنلاین، در...</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  \\\n",
       "63420  شنبه ۳۱ اردیبهشت ۱۴۰۱ - ۱۰:۴۲   \n",
       "23850   چهارشنبه ۲۴ آذر ۱۴۰۰ - ۰۸:۵۴   \n",
       "23876    سه‌شنبه ۲۳ آذر ۱۴۰۰ - ۰۹:۵۶   \n",
       "26393   جمعه ۲۶ فروردین ۱۴۰۱ - ۰۸:۱۹   \n",
       "23971    پنجشنبه ۱۸ آذر ۱۴۰۰ - ۰۹:۲۷   \n",
       "\n",
       "                                                   title  \\\n",
       "63420           معرفی بهترین گوشی تا قیمت ۸ میلیون تومان   \n",
       "23850  جدول | جدیدترین قیمت گوشی اپل در بازار | حداقل...   \n",
       "23876  جدول | جدیدترین قیمت انواع گوشی‌ | ارزان‌ترین ...   \n",
       "26393  قیمت‌های عجیب انواع گوشی آیفون | موبایل ۶۰ میل...   \n",
       "23971  جدول | جدیدترین قیمت انواع گوشی‌ | معرفی گران‌...   \n",
       "\n",
       "                                                   intro  \\\n",
       "63420  شما می توانید با بررسی معروف ترین و بهترین گوش...   \n",
       "23850  در این گزارش قیمت انواع گوشی اپل موجود در بازا...   \n",
       "23876  در این گزارش قیمت انواع گوشی شیائومی در بازار ...   \n",
       "26393  در بازار موبایل انواع گوشی‌های آیفون ۱۳ و ۱۲ ر...   \n",
       "23971  پرچمدار گران‌ترین گوشی‌ در بازار موبایل گوشی س...   \n",
       "\n",
       "                                                    body category  \n",
       "63420  از سوی دیگر در سال های اخیر قیمت مربوط به گوشی...       IT  \n",
       "23850  به گزارش همشهری آنلاین به نقل از باشگاه خبرنگا...  Economy  \n",
       "23876  به گزارش همشهری آنلاین به نقل از باشگاه خبرنگا...  Economy  \n",
       "26393  به گزارش همشهری آنلاین به نقل از خبرآنلاین، در...  Economy  \n",
       "23971  به گزارش همشهری آنلاین به نقل از خبرآنلاین، در...  Economy  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastText_model.predict_query_vector(qu, dataset, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "t76h-25dj39y"
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        از این که وقت ارزشمند خود را برای بررسی این نوتبوک صرف کرده‌اید، صمیمانه سپاسگزاریم. (:\n",
    "    </p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "News-Search-Engine.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
