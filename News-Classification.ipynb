{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "0ukP3ituDSCG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"center\">\n",
    "    <h3>\n",
    "        بسم الله الرحمن الرحیم\n",
    "    </h3>\n",
    "    <br>\n",
    "    <h1>\n",
    "        <strong>\n",
    "            بازیابی پیشرفته اطلاعات\n",
    "        </strong>\n",
    "    </h1>\n",
    "    <h2>\n",
    "        <strong>\n",
    "            تمرین چهارم (دسته‌بندی اخبار)\n",
    "        </strong>\n",
    "    </h2>\n",
    "    <br>\n",
    "    <h3>\n",
    "        محمد هجری - ٩٨١٠٦١٥٦\n",
    "        <br><br>\n",
    "        ارشان دلیلی - ٩٨١٠٥٧٥١\n",
    "        <br><br>\n",
    "        سروش جهان‌زاد - ٩٨١٠٠٣٨٩\n",
    "    </h3>\n",
    "    <br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cEdo5ea3j39X"
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        دسترسی به داده‌ها و مدل‌های ذخیره شده\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        به دلیل حجم بالای فایل‌های ذخیره شده، از قرار دادن آن‌ها در پوشه نوتبوک صرف نظر کرده و با اجرای قطعه کد زیر، از طریق گوگل درایو به آن‌ها دسترسی پیدا می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vcuDlf6j39Y",
    "outputId": "0e379dab-0d3a-4d06-c1c8-6710a4d0277c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/University/Term 6/MIR/Homeworks/HW3\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# address = \"University/Term 6/MIR/Homeworks/HW3\"\n",
    "# sys.path.append(f\"/content/drive/My Drive/{address}\")\n",
    "# %cd /content/drive/My\\ Drive/$address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IR0F_wAbDSCJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        نصب و دسترسی به کتابخانه‌های مورد نیاز\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای دو قطعه کد زیر، کتابخانه‌هایی که از آن‌ها در این تمرین استفاده شده است، نصب و قابل استفاده می‌شوند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IfxCzBqoDSCK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: hazm in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk==3.3->hazm) (1.16.0)\n",
      "Requirement already satisfied: unidecode in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (4.19.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (1.11.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (1.8.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (3.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sentence_transformers) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk->sentence_transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torchvision->sentence_transformers) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\mohammad\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.7.2)\n",
      "Collecting faiss-gpu\n",
      "  Using cached faiss-gpu-1.7.1.post2.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz#sha256=877478752c03678fd9b9553e4ffadca82cd337bba9bb6a939aa1c6ea561a7a58 (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz#sha256=877478752c03678fd9b9553e4ffadca82cd337bba9bb6a939aa1c6ea561a7a58 has inconsistent name: filename has 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.1.post1.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz#sha256=4e71f6ed035df0fee0eb40a35ce819c3f295116df15f513f18f40be8ac99be1f (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz#sha256=4e71f6ed035df0fee0eb40a35ce819c3f295116df15f513f18f40be8ac99be1f has inconsistent name: filename has 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.1.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz#sha256=78b8495efd00e5e25fd0046a652f5b4587af0aebaf006fff73ac66e9610ba9fc (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz#sha256=78b8495efd00e5e25fd0046a652f5b4587af0aebaf006fff73ac66e9610ba9fc has inconsistent name: filename has 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.7.0.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz#sha256=9dd7f92c10a04894e2b6065eb2274aa73705381327f34da5aae5da66523a2ca6 (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz#sha256=9dd7f92c10a04894e2b6065eb2274aa73705381327f34da5aae5da66523a2ca6 has inconsistent name: filename has 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.5.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz#sha256=e08fadbfa53d1ebc6657d36ef100f12b10ffff9cdb5834fa480264daf5dfc6f9 (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz#sha256=e08fadbfa53d1ebc6657d36ef100f12b10ffff9cdb5834fa480264daf5dfc6f9 has inconsistent name: filename has 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.4.post2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz#sha256=5d2c428af67442d1cd0317f189f842a71326e7676134fc2ada40021f127a0fd4 (from https://pypi.org/simple/faiss-gpu/): Requested faiss-cpu from https://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz#sha256=5d2c428af67442d1cd0317f189f842a71326e7676134fc2ada40021f127a0fd4 has inconsistent name: filename has 'faiss-gpu', but metadata has 'faiss-cpu'\n",
      "  Using cached faiss-gpu-1.6.4.tar.gz (3.4 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [7 lines of output]\n",
      "  running egg_info\n",
      "  creating C:\\Users\\Mohammad\\AppData\\Local\\Temp\\pip-pip-egg-info-m99aqgln\\faiss_cpu.egg-info\n",
      "  writing C:\\Users\\Mohammad\\AppData\\Local\\Temp\\pip-pip-egg-info-m99aqgln\\faiss_cpu.egg-info\\PKG-INFO\n",
      "  writing dependency_links to C:\\Users\\Mohammad\\AppData\\Local\\Temp\\pip-pip-egg-info-m99aqgln\\faiss_cpu.egg-info\\dependency_links.txt\n",
      "  writing top-level names to C:\\Users\\Mohammad\\AppData\\Local\\Temp\\pip-pip-egg-info-m99aqgln\\faiss_cpu.egg-info\\top_level.txt\n",
      "  writing manifest file 'C:\\Users\\Mohammad\\AppData\\Local\\Temp\\pip-pip-egg-info-m99aqgln\\faiss_cpu.egg-info\\SOURCES.txt'\n",
      "  error: package directory 'C:\\Users\\Mohammad\\AppData\\Local\\Temp\\pip-install-bvjb5dfd\\faiss-gpu_e4a11c9a5c2f4cdbb0584577a70d7a17\\faiss\\faiss\\python' does not exist\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install hazm\n",
    "!pip install unidecode\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sIglKdxHDSCM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import hazm\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from IPython.display import display\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aNJYaFC1DSCM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ١. دریافت داده‌ها\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این تمرین، بیش از ٦٨ هزار خبر از\n",
    "        <a href=\"https://www.hamshahrionline.ir/\"> وب‌سایت همشهری‌آنلاین </a>\n",
    "        گردآوری شده که در ١٠ دسته‌ی سیاسی، جهانی، اقتصادی، اجتماعی، شهری، ورزشی، علمی، فرهنگی، فناوری اطلاعات و مهارت‌های زندگی طبقه‌بندی شده‌اند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I5D13VU7DSCN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    'Politics': 'سیاسی',\n",
    "    'World': 'جهانی',\n",
    "    'Economy': 'اقتصادی',\n",
    "    'Society': 'اجتماعی',\n",
    "    'City': 'شهری',\n",
    "    'Sport': 'ورزشی',\n",
    "    'Science': 'علمی',\n",
    "    'Culture': 'فرهنگی',\n",
    "    'IT': 'فناوری اطلاعات',\n",
    "    'LifeSkills': 'مهارت‌های زندگی',\n",
    "}\n",
    "\n",
    "CATEGORIES_CLASSES = {\n",
    "    'Politics': 0,\n",
    "    'World': 1,\n",
    "    'Economy': 2,\n",
    "    'Society': 3,\n",
    "    'City': 4,\n",
    "    'Sport': 5,\n",
    "    'Science': 6,\n",
    "    'Culture': 7,\n",
    "    'IT': 8,\n",
    "    'LifeSkills': 9,\n",
    "}\n",
    "\n",
    "CLASSES_CATEGORIES = {v: k for k, v in CATEGORIES_CLASSES.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gviMdkLQDSCN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        برای دریافت داده‌ها یک ماژول Scraper ساخته‌ایم که اخبار مربوط به ١٠ دسته‌ی مذکور را در بازه‌ی زمانی تعیین شده، کراول کرده و در فایل dataset.zip ذخیره و فشرده سازی می‌کند. کد مربوط به این ماژول را در زیر مشاهده می‌کنید.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zlmFrCEHDSCO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "\n",
    "    def __init__(self, current_year, current_month):\n",
    "        self.current_year = current_year\n",
    "        self.current_month = current_month\n",
    "\n",
    "    def get_URL_content(self, URL):\n",
    "        while True:\n",
    "            try:\n",
    "                return requests.get(URL, timeout=5).content\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def generate_page_URL(self, page_index, category, year, month):\n",
    "        tp = {'Politics': 6, 'World': 11, 'Economy': 10, 'Society': 5, 'City': 7,\n",
    "              'Sport': 9, 'Science': 20, 'Culture': 26, 'IT': 718, 'LifeSkills': 21}[category]\n",
    "        return f'https://www.hamshahrionline.ir/archive?pi={page_index}&tp={tp}&ty=1&ms=0&mn={month}&yr={year}'\n",
    "\n",
    "    def get_page_URLs_by_time(self, category, year, month):\n",
    "        URLs = []\n",
    "        page_index = 1\n",
    "        while True:\n",
    "            URL = self.generate_page_URL(page_index, category, year, month)\n",
    "            content = self.get_URL_content(URL)\n",
    "            if re.findall('pagination', str(content)):\n",
    "                URLs.append(URL)\n",
    "                page_index += 1\n",
    "            else:\n",
    "                break\n",
    "        return URLs\n",
    "\n",
    "    def get_page_URLs_since(self, category, year, month):\n",
    "        URLs = []\n",
    "        with tqdm() as pbar:\n",
    "            while True:\n",
    "                if month > 12:\n",
    "                    month = 1\n",
    "                    year += 1\n",
    "                pbar.set_description(f'[{category}] [Extracting page URLs] [Date: {year}/{month}]')\n",
    "                URLs_by_time = self.get_page_URLs_by_time(category, year, month)\n",
    "                if URLs_by_time:\n",
    "                    for URL in URLs_by_time:\n",
    "                        URLs.append(URL)\n",
    "                    month += 1\n",
    "                elif self.current_year > year or (self.current_year == year and self.current_month > month):\n",
    "                    month += 1\n",
    "                else:\n",
    "                    break\n",
    "        return URLs\n",
    "\n",
    "    def get_news_URLs_since(self, category, year, month):\n",
    "        news_URLs = []\n",
    "        page_URLs = self.get_page_URLs_since(category, year, month)\n",
    "        with tqdm(page_URLs) as pbar:\n",
    "            for page_URL in pbar:\n",
    "                content = self.get_URL_content(page_URL)\n",
    "                soup = BeautifulSoup(content, 'html5lib')\n",
    "                for item in soup.findAll('li', attrs={'class': 'news'}):\n",
    "                    URL = item.find('div', attrs={'class': 'desc'}).find('h3').find('a')['href']\n",
    "                    URL = 'https://www.hamshahrionline.ir' + URL\n",
    "                    news_URLs.append(URL)\n",
    "                pbar.set_description(f'[{category}] [Extracting news URLs] [{len(news_URLs)} news until now]')\n",
    "        return news_URLs\n",
    "\n",
    "    def parse_news(self, URL, category):\n",
    "        try:\n",
    "            content = self.get_URL_content(URL)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            date = soup.find('div', {'class': 'col-6 col-sm-4 col-xl-4 item-date'}).span.text.strip()\n",
    "            title = soup.find('div', {'class': 'item-title'}).h1.text.strip()\n",
    "            intro = soup.find('p', {'class': 'introtext', 'itemprop': 'description'}).text.strip()\n",
    "            body = soup.find('div', {'class': 'item-text', 'itemprop': 'articleBody'}).text.strip()\n",
    "            return {\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'intro': intro,\n",
    "                'body': body,\n",
    "                'category': category,\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def scrape(self, from_year, from_month):\n",
    "        categories = ['Politics', 'World', 'Economy', 'Society', 'City',\n",
    "                      'Sport', 'Science', 'Culture', 'IT', 'LifeSkills']\n",
    "        news = []\n",
    "        for category in categories:\n",
    "            URLs = self.get_news_URLs_since(category, from_year, from_month)\n",
    "            with tqdm(URLs) as pbar:\n",
    "                pbar.set_description(f'[{category}] [Scraping news]')\n",
    "                for URL in pbar:\n",
    "                    news.append(self.parse_news(URL, category))\n",
    "        news = list(filter(None, news))\n",
    "        pd.DataFrame(news).to_csv(f'dataset.csv', encoding='utf-8')\n",
    "        with zipfile.ZipFile('dataset.zip', 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.write('dataset.csv')\n",
    "        os.remove('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Kt-SS3ReDSCP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Scraper ایجاد شده و شروع به دریافت و ذخیره‌سازی داده‌ها می‌کند. خبرهای دریافت شده همگی مربوط به قرن جدید، از سال ١٤٠٠ به بعد هستند.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uiwTDzE6DSCP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scraper = Scraper(current_year=1401, current_month=3)\n",
    "scraper.scrape(from_year=1400, from_month=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2ynwZYeLDSCQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        بعد از ذخیره شدن داده‌ها در فایل فشرده dataset.zip، آن‌ها را از این فایل استخراج کرده و وارد برنامه می‌کنیم. با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bFUFTo7mDSCQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset_from_file():\n",
    "    dataset = []\n",
    "    with zipfile.ZipFile('dataset.zip', 'r') as zip_file:\n",
    "        zip_file.extractall()\n",
    "    with open('dataset.csv', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            data = dict(zip(header[1:], row[1:]))\n",
    "            dataset.append(data)\n",
    "    os.remove('dataset.csv')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame(read_dataset_from_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OkLqdbKkDSCR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "    با اجرای قطعه کد زیر، تعداد خبرهای هر دسته و تعداد کل خبرها را می‌توان مشاهده کرد.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "ktN_vJjlDSCR",
    "outputId": "edf2e129-d901-4de8-9b56-2383578c4585",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               دسته  تعداد\n0          کل خبرها  68362\n1             سیاسی  15798\n2             جهانی   2895\n3           اقتصادی   8900\n4           اجتماعی  13585\n5              شهری   3853\n6             ورزشی   8348\n7              علمی   3190\n8            فرهنگی   6512\n9    فناوری اطلاعات    437\n10  مهارت‌های زندگی   4844",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>دسته</th>\n      <th>تعداد</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>کل خبرها</td>\n      <td>68362</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>سیاسی</td>\n      <td>15798</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>جهانی</td>\n      <td>2895</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>اقتصادی</td>\n      <td>8900</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اجتماعی</td>\n      <td>13585</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>شهری</td>\n      <td>3853</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ورزشی</td>\n      <td>8348</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>علمی</td>\n      <td>3190</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>فرهنگی</td>\n      <td>6512</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>فناوری اطلاعات</td>\n      <td>437</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>مهارت‌های زندگی</td>\n      <td>4844</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_dataset_info():\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    length_dict = {key: 0 for key in CATEGORIES.keys()}\n",
    "    for _, data in dataset.iterrows():\n",
    "        length_dict[data['category']] += 1\n",
    "\n",
    "    df_dict = {\n",
    "        'دسته': CATEGORIES.values(),\n",
    "        'تعداد': length_dict.values(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    df.index += 1\n",
    "    df.loc[0] = ['کل خبرها', len(dataset)]\n",
    "    df = df.sort_index()\n",
    "    display(df)\n",
    "\n",
    "\n",
    "display_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Bah1P3lNDSCS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ٢. پیش پردازش اولیه‌ی متن\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        ابزار مورد استفاده برای پیش‌پردازش متن ورودی به صورت ماژولار طراحی شده است؛ به طوری که با صدا زدن تابع preprocess از آن، متن داده شده با عبور از یک خط لوله به صورت مرحله به مرحله تغییر می‌کند تا به یک ساختار استاندارد برسد. این مراحل عبارتند از:\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        نرمال سازی داده‌ها (normalize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف لینک‌ها (remove_links)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف نشانه‌های نگارشی (remove_punctuations)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        واحد سازی داده‌ها (word_tokenize)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف کلمات نامعتبر (remove_invalid_words)\n",
    "    </li>\n",
    "    <li style='direction:rtl;text-align:justify;'>\n",
    "        حذف ایست‌واژه‌ها (remove_stopwords)\n",
    "    </li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SjtCqkWsDSCS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, stopwords_path):\n",
    "        self.stopwords = []\n",
    "        with open(stopwords_path, encoding='utf-8') as file:\n",
    "            self.stopwords = file.read().split()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_links(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        words = self.word_tokenize(text)\n",
    "        words = self.remove_invalid_words(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    def normalize(self, text):\n",
    "        return hazm.Normalizer().normalize(text)\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        patterns = ['\\S*http\\S*', '\\S*www\\S*', '\\S+\\.ir\\S*', '\\S+\\.com\\S*', '\\S+\\.org\\S*', '\\S*@\\S*']\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        return re.sub(f'[{punctuation}؟،٪×÷»«]+', '', text)\n",
    "\n",
    "    def word_tokenize(self, text):\n",
    "        return hazm.word_tokenize(text)\n",
    "\n",
    "    def remove_invalid_words(self, words):\n",
    "        return [word for word in words if len(word) > 3 or re.match('^[\\u0600-\\u06FF]{2,3}$', word)]\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        return [word for word in words if word not in self.stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BjH18UKt0b9O"
   },
   "outputs": [],
   "source": [
    "def save_preprocessed_texts(texts, path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(texts, file)\n",
    "\n",
    "\n",
    "def load_preprocessed_texts(path=\"Preprocessed_texts.pickle\"):\n",
    "    with open(path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def data_to_text(data):\n",
    "    return ' '.join([data['title'], data['intro'], data['body']]).lower()\n",
    "\n",
    "\n",
    "def get_preprocessed_texts(dataset, preprocessor, mode, save=False):\n",
    "    preprocessed_texts = []\n",
    "    if mode == 'process':\n",
    "        texts = [data_to_text(data) for _, data in dataset.iterrows()]\n",
    "        preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    if mode == 'load':\n",
    "        preprocessed_texts = load_preprocessed_texts()\n",
    "    if save:\n",
    "        save_preprocessed_texts(preprocessed_texts)\n",
    "    return preprocessed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SQqYXKb1DSCT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با اجرای قطعه کد زیر، یک instance از ماژول Preprocessor ایجاد کرده و شروع به پیش پردازش داده‌ها می‌کنیم، یا داده‌های پیش‌پردازش‌شده‌ی ذخیره‌شده را از فایل مربوطه بازیابی می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8cc5G2zWDSCT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68362/68362 [07:50<00:00, 145.35it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(stopwords_path='stopwords.txt')\n",
    "preprocessed_texts = get_preprocessed_texts(dataset, preprocessor, mode='load', save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2ZLEketNj39j",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        با توجه به این که حجم مجموعه داده‌ها باعث ایجاد محدودیت حافظه و زمان برای مدل tf-idf می‌شود، ناچار هستیم کسری از مجموعه‌ی داده‌ها را به عنوان ورودی برای یادگیری به آن بدهیم. با اجرای قطعه کد زیر، یک مجموعه داده‌ی کوچک‌تر از روی مجموعه داده‌ی اصلی ایجاد می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKMAe5bwDSCU",
    "outputId": "2fc00f8a-3e97-4bf2-b4dd-fe4372533e75",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9437/9437 [00:55<00:00, 169.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_mini_dataset(len_each_category=1000):\n",
    "    global CATEGORIES, dataset\n",
    "\n",
    "    mini_dataset = []\n",
    "    for category in CATEGORIES.keys():\n",
    "        dataset_by_category = dataset.loc[dataset['category'] == category]\n",
    "        length = min(len_each_category, dataset_by_category.shape[0])\n",
    "        mini_dataset.append(dataset_by_category.sample(length, random_state=1))\n",
    "\n",
    "    mini_dataset = pd.concat(mini_dataset).reset_index(drop=True)\n",
    "    texts = [data_to_text(data) for _, data in mini_dataset.iterrows()]\n",
    "    mini_preprocessed_texts = [preprocessor.preprocess(text) for text in tqdm(texts)]\n",
    "    return mini_dataset, mini_preprocessed_texts\n",
    "\n",
    "\n",
    "# Almost 10K News\n",
    "mini_dataset, mini_preprocessed_texts = get_mini_dataset()\n",
    "mini_dataset['category_code'] = mini_dataset['category'].apply(lambda x: CATEGORIES_CLASSES[x])\n",
    "labels = mini_dataset['category_code'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "    <h3 style='direction:rtl;text-align:justify;'>\n",
    "        ٣. دسته‌بندی اخبار بر اساس بر چسب خبر\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این مرحله با استفاده از دو مدل متفاوت زبانی، یکی مبتنی بر روش سنتی رگرسیون لجستیک و دیگری بر پایه ترنسفرمر، شروع به یادگیری روی خبرهای استخراج شده می‌کنیم و در انتها، نتیجه اجرای ده کوئری را بر روی هر یک مشاهده می‌کنیم. در نهایت، ارزیابی‌های مورد بحث در داک تمرین را بر روی داده‌های تست انجام می‌دهیم.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        کوئری‌های مورد استفاده در این بخش در زیر آمده‌اند.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "QUERIES = [\n",
    "    \"نتیجه توافق برجام\",\n",
    "    \"حمله ارتش روسیه به اوکراین\",\n",
    "    \"افزایش نرخ تورم در کشور\",\n",
    "    \"آمار فوتی‌های کرونا\",\n",
    "    \"اقدامات شهرداری تهران\",\n",
    "    \"صعود ایران به جام جهانی\",\n",
    "    \"بیماری آبله میمونی\",\n",
    "    \"جشنواره فیلم فجر\",\n",
    "    \"رونمایی از گوشی جدید اپل\",\n",
    "    \"روش پخت غذا\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "    <h4 style='direction:rtl;text-align:justify;'>\n",
    "        ١. مدل مبتنی بر روش Logistic Regression\n",
    "    </h4>\n",
    "</div>\n",
    "\n",
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این بخش از کلاس TfidfVectorizer از کتاب‌خانه‌ی sklearn استفاده کرده و مجموعه کلمات یکتای موجود در عناوین خبرها را به عنوان ورودی به آن می‌دهیم. این کلمات یکتا را از روی فایل ذخیره شده‌ی vocabulary.pickle می‌خوانیم. در ادامه، یک مدل ایجاد کرده و بردار embedding عنوان هر خبر را به روش tf-idf به دست می‌آوریم. سپس، داده‌هایمان را به دو مجموعه train و test افراز می‌کنیم. حال، با استفاده از کلاس LogisticRegression کتاب‌خانه‌ی sklearn شروع به یادگیری داده‌های train می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "    def fit_transform_vectorizer(self, dataset):\n",
    "        vectors = self.vectorizer.fit_transform(list(map(lambda doc: ' '.join(doc), dataset)))\n",
    "        dense_vectors = vectors.todense().tolist()\n",
    "        return np.array(dense_vectors)\n",
    "\n",
    "    def save_TF_IDF_model(self, path=\"TF_IDF_model.pickle\"):\n",
    "        with open(path, \"wb\") as file:\n",
    "            pickle.dump(self.vectors.todense().tolist(), file)\n",
    "\n",
    "    def load_TF_IDF_model(self, path=\"TF_IDF_model.pickle\"):\n",
    "        with open(path, \"rb\") as file:\n",
    "            return pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "with open('vocabulary.pickle', \"rb\") as file:\n",
    "    vocabulary = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "TF_IDF_model = TF_IDF(vocabulary)\n",
    "vectors = TF_IDF_model.fit_transform_vectorizer(mini_preprocessed_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors, labels, test_size=0.15, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000, random_state=0)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=0)</pre></div></div></div></div></div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=1000, random_state=0)\n",
    "classifier.fit(train_vectors, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این قسمت، دسته‌ی هر یک از کوئری‌های مشخص شده را با استفاده از مدل یادگیری پیش‌بینی می‌کنیم.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "                        Query Predicted Category  Probability\n0           نتیجه توافق برجام           Politics     0.386296\n1  حمله ارتش روسیه به اوکراین              World     0.926111\n2     افزایش نرخ تورم در کشور            Economy     0.456680\n3         آمار فوتی‌های کرونا            Society     0.490125\n4       اقدامات شهرداری تهران               City     0.995976\n5     صعود ایران به جام جهانی              Sport     0.644007\n6          بیماری آبله میمونی            Science     0.538684\n7            جشنواره فیلم فجر            Culture     0.963572\n8    رونمایی از گوشی جدید اپل                 IT     0.773893\n9                 روش پخت غذا         LifeSkills     0.586952",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Query</th>\n      <th>Predicted Category</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>نتیجه توافق برجام</td>\n      <td>Politics</td>\n      <td>0.386296</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>حمله ارتش روسیه به اوکراین</td>\n      <td>World</td>\n      <td>0.926111</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>افزایش نرخ تورم در کشور</td>\n      <td>Economy</td>\n      <td>0.456680</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>آمار فوتی‌های کرونا</td>\n      <td>Society</td>\n      <td>0.490125</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اقدامات شهرداری تهران</td>\n      <td>City</td>\n      <td>0.995976</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>صعود ایران به جام جهانی</td>\n      <td>Sport</td>\n      <td>0.644007</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>بیماری آبله میمونی</td>\n      <td>Science</td>\n      <td>0.538684</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>جشنواره فیلم فجر</td>\n      <td>Culture</td>\n      <td>0.963572</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>رونمایی از گوشی جدید اپل</td>\n      <td>IT</td>\n      <td>0.773893</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>روش پخت غذا</td>\n      <td>LifeSkills</td>\n      <td>0.586952</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries_class_prediction = []\n",
    "\n",
    "for query in QUERIES:\n",
    "    preprocessed_query = ' '.join(preprocessor.preprocess(query))\n",
    "    query_embed = TF_IDF_model.vectorizer.transform([preprocessed_query])\n",
    "    predicted_class_code = classifier.predict(query_embed)[0]\n",
    "    probability = classifier.predict_proba(query_embed)[0][predicted_class_code]\n",
    "    queries_class_prediction.append({\n",
    "        'Query': query,\n",
    "        'Predicted Category': CLASSES_CATEGORIES[predicted_class_code],\n",
    "        'Probability': probability\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(queries_class_prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"auto\" align=\"justify\">\n",
    "    <p style='direction:rtl;text-align:justify;'>\n",
    "        در این قسمت، بررسی می‌کنیم که مدل یادگیری‌مان بر روی داده‌های test چه عملکردی دارد. با اجرای قطعه کد زیر، معیار ارزیابی F1-Macro و نیز accuracy و ماتریس درهم‌ریختگی محاسبه شده و نمایش داده می‌شوند.\n",
    "    </p>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accurracy: 0.849\n",
      "\n",
      "F1-Macro: 0.846\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "            Politics  World  Economy  Society  City  Sport  Science  Culture  \\\nPolitics         114     13        7       10     2      1        1        3   \nWorld              3    111        2        5     0      0        5        1   \nEconomy            4      0      142        7     2      0        1        0   \nSociety           10      0        8      122    12      0        2        4   \nCity               3      1        2        7   125      0        1        2   \nSport              2      1        0        0     0    161        0        1   \nScience            0      6        1        2     0      0      116        4   \nCulture            1      3        1        0     1      0        0      130   \nIT                 2      5        2        0     0      1        4        0   \nLifeSkills         1      3        4        3     1      2        5        3   \n\n            IT  LifeSkills  \nPolitics     2           0  \nWorld        1           6  \nEconomy      2           2  \nSociety      0           3  \nCity         0           0  \nSport        0           2  \nScience      6           9  \nCulture      0           2  \nIT          57           0  \nLifeSkills   1         124  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Politics</th>\n      <th>World</th>\n      <th>Economy</th>\n      <th>Society</th>\n      <th>City</th>\n      <th>Sport</th>\n      <th>Science</th>\n      <th>Culture</th>\n      <th>IT</th>\n      <th>LifeSkills</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Politics</th>\n      <td>114</td>\n      <td>13</td>\n      <td>7</td>\n      <td>10</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>World</th>\n      <td>3</td>\n      <td>111</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>Economy</th>\n      <td>4</td>\n      <td>0</td>\n      <td>142</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>Society</th>\n      <td>10</td>\n      <td>0</td>\n      <td>8</td>\n      <td>122</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>City</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>7</td>\n      <td>125</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Sport</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>161</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>Science</th>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>116</td>\n      <td>4</td>\n      <td>6</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>Culture</th>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>130</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>IT</th>\n      <td>2</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>57</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>LifeSkills</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n      <td>3</td>\n      <td>1</td>\n      <td>124</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_acc = classifier.score(test_vectors, test_labels)\n",
    "print(f\"Mean Accurracy: {round(mean_acc, 3)}\\n\")\n",
    "\n",
    "predicted_labels = classifier.predict(test_vectors)\n",
    "f1_macro = f1_score(test_labels, predicted_labels, labels=np.arange(10), average='macro')\n",
    "print(f\"F1-Macro: {round(f1_macro, 3)}\\n\")\n",
    "\n",
    "confusion_mat = confusion_matrix(test_labels, predicted_labels, labels=np.arange(10))\n",
    "confusion_mat_df = pd.DataFrame(confusion_mat).rename(columns=CLASSES_CATEGORIES, index=CLASSES_CATEGORIES)\n",
    "display(confusion_mat_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "News-Search-Engine.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}